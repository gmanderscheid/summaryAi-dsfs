{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aa09d8",
   "metadata": {},
   "source": [
    "# Cours Résumé IA M06D07 - Machine Learning : Sélection et Évaluation de Modèles\n",
    "\n",
    "---\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/M06D07_Datascience.m4a\n",
    "\n",
    "## Introduction et contexte\n",
    "\n",
    "Ce module couvre les aspects fondamentaux de la sélection et de l'évaluation de modèles en machine learning, avec un focus particulier sur les méthodes de validation croisée, les intervalles de confiance, et l'évaluation robuste des performances.\n",
    "\n",
    "### Principe du biais pessimiste\n",
    "\n",
    "Les performances observées sur les données d'entraînement seront **toujours meilleures** que celles en production réelle. Cette réalité impose l'adoption d'un **biais pessimiste** lors de l'évaluation des modèles.\n",
    "\n",
    "**Exemple pratique :** Si un modèle affiche 75% d'accuracy, il convient d'appliquer une marge de sécurité (comme 10%) pour anticiper qu'en réalité, il pourrait n'atteindre que 65%.\n",
    "\n",
    "### Facteurs expliquant les différences de performance\n",
    "\n",
    "- **L'overfitting** : un modèle à 75% sur test mais 95% sur train pourrait moins bien performer qu'un modèle à 70% sur les deux\n",
    "- **La variance des scores** : un modèle avec une déviation standard élevée dans ses performances de validation croisée est moins fiable\n",
    "\n",
    "---\n",
    "\n",
    "## Théorème Central Limite et intervalles de confiance\n",
    "\n",
    "### Fondements théoriques\n",
    "\n",
    "Le **Théorème Central Limite** constitue la base mathématique permettant de calculer des intervalles de confiance. Ce théorème établit que la moyenne de tout échantillon de plus de 30 valeurs appartient à une distribution normale, peu importe la distribution originale des données.\n",
    "\n",
    "**Analogie des dés :** Imaginez lancer un dé 30 fois et calculer la moyenne. Répétez cette opération 200 fois avec différents dés (même pipés). La distribution de ces 200 moyennes suivra une loi normale, même si la distribution individuelle des lancers ne l'était pas.\n",
    "\n",
    "### Application pratique aux modèles ML\n",
    "\n",
    "Pour un modèle avec des scores d'accuracy obtenus par validation croisée, l'intervalle de confiance se calcule ainsi :\n",
    "\n",
    "$$\n",
    "\\text{Intervalle} = \\text{moyenne} \\pm Z_{1-\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "Où :\n",
    "- **Z = 1.96** pour un niveau de confiance de 95%\n",
    "- **σ** = déviation standard des scores\n",
    "- **n** = nombre de scores obtenus\n",
    "\n",
    "**Exemple concret :** Si votre modèle obtient une accuracy moyenne de 0.85 avec σ = 0.03 sur 40 mesures, vous pouvez affirmer être sûr à 95% que la vraie accuracy se situe entre 0.84 et 0.86.\n",
    "\n",
    "---\n",
    "\n",
    "## Méthodes de validation\n",
    "\n",
    "### Train-Test Split classique\n",
    "\n",
    "La méthode la plus simple divise les données en 70-80% pour l'entraînement et 20-30% pour les tests. La **stratification** garantit que la distribution des classes reste identique entre train et test, particulièrement crucial pour des données déséquilibrées.\n",
    "\n",
    "**Limite principale :** Cette méthode ne fournit qu'une seule mesure de performance, potentiellement biaisée par le hasard du split.\n",
    "\n",
    "### Validation croisée K-Fold\n",
    "\n",
    "La méthode standard divise les données en K parties égales (généralement 5 ou 10).\n",
    "\n",
    "**Pourquoi 5 folds ?** C'est un compromis optimal entre temps de calcul et robustesse statistique. Moins de 4 folds devient insuffisant, plus de 10 devient coûteux sans gain significatif.\n",
    "\n",
    "**Avantage concret :** Sur un dataset de 1000 exemples avec 5-fold CV, chaque modèle s'entraîne sur 800 exemples et se teste sur 200, répété 5 fois avec des partitions différentes.\n",
    "\n",
    "### Monte Carlo Cross-Validation\n",
    "\n",
    "Cette méthode sophistiquée répète le train-test split plusieurs centaines de fois avec des partitions aléatoires différentes. Contrairement au K-fold, les mêmes données peuvent apparaître plusieurs fois dans différents sets de test ou ne jamais y apparaître.\n",
    "\n",
    "**Algorithme :**\n",
    "1. Répéter B fois (ex: 500 itérations)\n",
    "2. À chaque itération : diviser aléatoirement en train/test\n",
    "3. Entraîner le modèle sur train, évaluer sur test\n",
    "4. Calculer la moyenne des B scores obtenus\n",
    "\n",
    "**Avantage principal :** Fournit une estimation plus robuste avec une variance plus faible que K-fold CV, mais au coût de B fois plus d'entraînements.\n",
    "\n",
    "---\n",
    "\n",
    "## Équilibre biais-variance\n",
    "\n",
    "### Concepts fondamentaux\n",
    "\n",
    "L'analogie de l'artillerie illustre parfaitement ces concepts :\n",
    "- **Faible biais + Faible variance** : Tirs groupés près de la cible (modèle idéal)\n",
    "- **Fort biais + Faible variance** : Tirs groupés mais loin de la cible (underfitting)\n",
    "- **Faible biais + Forte variance** : Tirs dispersés autour de la cible (overfitting)\n",
    "- **Fort biais + Forte variance** : Tirs dispersés et loin (pire cas)\n",
    "\n",
    "**En pratique :** Un Random Forest avec peu d'arbres aura forte variance, avec trop d'arbres risque l'overfitting. Un modèle linéaire simple aura fort biais sur des données complexes.\n",
    "\n",
    "### Implications concrètes\n",
    "\n",
    "Plus la complexité du modèle augmente :\n",
    "- **Biais diminue** : meilleure capacité à capturer les patterns complexes\n",
    "- **Variance augmente** : plus sensible aux variations dans les données d'entraînement\n",
    "\n",
    "---\n",
    "\n",
    "## Méthodes de validation avancées\n",
    "\n",
    "### Three-Way Holdout\n",
    "\n",
    "Cette méthode divise les données en trois parties : train/validation/test. Utilisée quand on optimise intensivement les hyperparamètres sur plusieurs semaines, elle évite le sur-ajustement au set de test.\n",
    "\n",
    "**Processus :**\n",
    "1. **Train (60%)** : Entraînement des modèles\n",
    "2. **Validation (20%)** : Optimisation des hyperparamètres\n",
    "3. **Test (20%)** : Évaluation finale uniquement\n",
    "\n",
    "### Validation croisée imbriquée (Nested CV)\n",
    "\n",
    "La méthode la plus rigoureuse pour la sélection de modèles. Elle utilise deux boucles de cross-validation :\n",
    "- **Boucle externe** : Évalue la performance générale\n",
    "- **Boucle interne** : Optimise les hyperparamètres\n",
    "\n",
    "**Exemple concret :** Avec 5-fold externe et 3-fold interne, vous entraînez 5×3 = 15 modèles pour chaque combinaison d'hyperparamètres.\n",
    "\n",
    "---\n",
    "\n",
    "## Courbes d'apprentissage et diagnostics\n",
    "\n",
    "### Learning Curves\n",
    "\n",
    "Les courbes d'apprentissage tracent la performance en fonction du nombre d'exemples d'entraînement. Elles révèlent :\n",
    "- **Underfitting** : Les courbes train et validation convergent vers un score faible\n",
    "- **Overfitting** : Large écart persistant entre train (élevé) et validation (faible)\n",
    "- **Modèle optimal** : Courbes qui se rapprochent vers un score élevé\n",
    "\n",
    "### Validation Curves\n",
    "\n",
    "Ces courbes montrent l'impact d'un hyperparamètre sur la performance. Exemple avec le paramètre C d'un SVM :\n",
    "- **C trop faible** : Underfitting (train et validation scores faibles)\n",
    "- **C optimal** : Scores élevés et proches\n",
    "- **C trop élevé** : Overfitting (écart train/validation)\n",
    "\n",
    "---\n",
    "\n",
    "## Optimisation du nombre de données\n",
    "\n",
    "Le cours démontre empiriquement qu'ajouter plus de données n'améliore pas toujours significativement les performances. Sur l'exemple présenté :\n",
    "- **Avec 2500 exemples** : accuracy = 0.75\n",
    "- **Avec 200 000 exemples** : accuracy = 0.76\n",
    "\n",
    "**Implication pratique :** Pour l'optimisation d'hyperparamètres, commencer avec un sous-échantillon permet d'économiser du temps de calcul sans perte significative de qualité.\n",
    "\n",
    "---\n",
    "\n",
    "## Tests statistiques pour comparaison de modèles\n",
    "\n",
    "### Tests recommandés\n",
    "\n",
    "**Test de McNemar :** Standard pour comparer deux classificateurs sur le même dataset\n",
    "\n",
    "**Test t apparié :** Pour comparer les moyennes de performance de deux modèles\n",
    "\n",
    "**Test de permutation :** Méthode non-paramétrique robuste pour comparaisons\n",
    "\n",
    "### Bootstrap et rééchantillonnage\n",
    "\n",
    "Le bootstrap génère plusieurs échantillons en tirant avec remise du dataset original. Cette technique permet d'estimer la distribution des performances et de calculer des intervalles de confiance sans suppositions sur la distribution sous-jacente.\n",
    "\n",
    "---\n",
    "\n",
    "## Métriques d'évaluation contextuelles\n",
    "\n",
    "### Choix des métriques\n",
    "\n",
    "Le cours souligne l'importance d'adapter les métriques au contexte business :\n",
    "- **Pour le client** : MAE (erreur absolue moyenne) en régression est plus parlante que R²\n",
    "- **Pour l'analyse technique** : Précision/Rappel, vrais/faux positifs sont plus informatifs que l'accuracy seule\n",
    "- **Accuracy** : Métrique la plus simple à vendre mais souvent insuffisante\n",
    "\n",
    "### R² et ses limitations\n",
    "\n",
    "Le coefficient de détermination R² compare votre modèle à une baseline (moyenne) :\n",
    "\n",
    "$$\n",
    "R² = 1 - \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\sum(\\text{vraie valeur} - \\text{prédiction})²}{\\sum(\\text{vraie valeur} - \\text{moyenne})²}\n",
    "$$\n",
    "\n",
    "**Problème :** Si la moyenne est une très mauvaise baseline, un R² élevé peut être trompeur.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommandations pratiques et workflow\n",
    "\n",
    "### Séquence optimale d'évaluation\n",
    "\n",
    "1. **Grid Search + CV** pour identifier les meilleurs hyperparamètres (coût modéré)\n",
    "2. **Monte Carlo CV** uniquement sur la combinaison optimale (coût élevé)\n",
    "3. **Three-way holdout** pour validation finale si nécessaire\n",
    "\n",
    "### Gestion des ressources computationnelles\n",
    "\n",
    "**Exemple concret :** Monte Carlo avec 100 itérations sur un large dataset peut nécessiter 100× plus de temps. Réservez cette méthode pour la validation finale du modèle sélectionné.\n",
    "\n",
    "### Documentation et traçabilité\n",
    "\n",
    "Maintenir la traçabilité des random_state pour pouvoir reproduire et analyser les résultats aberrants. Cette pratique est cruciale en environnement professionnel pour déboguer et expliquer les performances.\n",
    "\n",
    "---\n",
    "\n",
    "## Cas d'usage industriels\n",
    "\n",
    "### Contraintes de production\n",
    "\n",
    "En entreprise, la collecte de nouvelles colonnes de données peut prendre des semaines voire des mois. D'où l'importance de :\n",
    "- Définir précisément les données disponibles en production\n",
    "- Négocier l'accès aux données pertinentes dès le début du projet\n",
    "- Anticiper les décalages entre données d'entraînement et de production\n",
    "\n",
    "### Monitoring et maintenance\n",
    "\n",
    "L'évaluation continue en production nécessite de surveiller :\n",
    "- La dérive des distributions d'entrée\n",
    "- La cohérence des performances dans le temps\n",
    "- L'adéquation entre métriques techniques et objectifs business\n",
    "\n",
    "Cette approche méthodologique de l'évaluation de modèles garantit des déploiements robustes et des prédictions fiables en conditions réelles, tout en optimisant l'allocation des ressources computationnelles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c8521",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
