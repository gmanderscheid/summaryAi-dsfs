{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244b0854",
   "metadata": {},
   "source": [
    "# Cours Résumé IA M06D04 - Classification et Régression Logistique\n",
    "---\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/M06D04_DataScience.m4a\n",
    "\n",
    "## Introduction à la classification supervisée\n",
    "---\n",
    "\n",
    "### Objectif et distinction avec la régression\n",
    "\n",
    "Ce module explore la logique de la régression logistique et des modèles de classification supervisée, en s'appuyant sur des exemples concrets comme le dataset \"Barnet\".\n",
    "\n",
    "**Distinction fondamentale :**\n",
    "- **Régression linéaire** : prédit une valeur continue (ex: salaire en fonction de l'expérience)\n",
    "- **Classification** : prédit une appartenance à une classe discrète (ex: achat vs non-achat d'un produit)\n",
    "\n",
    "Cette différence conceptuelle nécessite des approches mathématiques et des métriques d'évaluation spécifiques.\n",
    "\n",
    "## La fonction sigmoïde : transformation probabiliste\n",
    "---\n",
    "\n",
    "### Définition mathématique\n",
    "\n",
    "La fonction sigmoïde transforme les sorties de la régression linéaire en probabilités comprises entre 0 et 1 :\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "où $x = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p$ représente la combinaison linéaire des variables d'entrée.\n",
    "\n",
    "**Propriétés clés :**\n",
    "- Fonction monotone croissante\n",
    "- Asymptotes horizontales en 0 et 1\n",
    "- Point d'inflexion en x = 0 (probabilité = 0.5)\n",
    "\n",
    "### Interprétation pratique\n",
    "\n",
    "**Exemple concret :** Si $\\sigma(x) = 0.9$, cela signifie \"90% de chance d'appartenance à la classe positive\".\n",
    "\n",
    "**Analogie :** La sigmoïde agit comme un filtre probabiliste : elle convertit une \"température\" brute (combinaison linéaire) en probabilité de risque comprise entre 0 (aucun risque) et 1 (risque maximal).\n",
    "\n",
    "### Rôle de l'exponentielle et du logarithme\n",
    "\n",
    "- **Exponentielle** : croissance très rapide, toujours positive\n",
    "- **Logarithme** : \"applatit\" les grands nombres, devient négatif pour les valeurs < 1\n",
    "\n",
    "**Exemple numérique :** $\\log(100\\,000\\,000) \\approx 20$, illustrant la compression logarithmique des ordres de grandeur.\n",
    "\n",
    "## Fonction de coût : log loss\n",
    "---\n",
    "\n",
    "### Principe et formulation\n",
    "\n",
    "La fonction de coût \"log loss\" (entropie croisée binaire) pénalise les prédictions incorrectes de façon asymétrique :\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i)]\n",
    "$$\n",
    "\n",
    "où $y_i \\in \\{0,1\\}$ est la vraie classe et $\\hat{p}_i$ la probabilité prédite.\n",
    "\n",
    "### Comportement de la pénalité\n",
    "\n",
    "- **Prédiction correcte et confiante** : coût très faible\n",
    "- **Prédiction incorrecte et confiante** : coût très élevé (tend vers l'infini)\n",
    "\n",
    "**Analogie :** Comme parier sur un match : miser correctement rapporte peu, mais miser massivement sur le mauvais résultat entraîne des pertes considérables.\n",
    "\n",
    "### Applications contextuelles\n",
    "\n",
    "**En médecine :** Le modèle doit minimiser les faux négatifs - il vaut mieux alerter à tort que manquer un cas de cancer.\n",
    "\n",
    "Cette philosophie influence directement le choix des seuils de décision et des métriques d'évaluation.\n",
    "\n",
    "## Gestion du déséquilibre des classes\n",
    "---\n",
    "\n",
    "### Problématique et solutions\n",
    "\n",
    "Quand les classes sont déséquilibrées (ex: 95% non-acheteurs, 5% acheteurs), le paramètre `class_weight` permet de rééquilibrer l'apprentissage en pondérant davantage la classe minoritaire.\n",
    "\n",
    "**Stratégies de pondération :**\n",
    "- `class_weight='balanced'` : pondération inverse de la fréquence\n",
    "- Pondération manuelle selon l'importance business\n",
    "\n",
    "**Exemple pratique :** En détection de fraude bancaire, les transactions frauduleuses sont rares mais critiques. La pondération des classes rend le modèle plus sensible à ces cas minoritaires essentiels.\n",
    "\n",
    "## Extension à la multi-classification\n",
    "---\n",
    "\n",
    "### Softmax : généralisation de la sigmoïde\n",
    "\n",
    "Pour plus de deux classes, la régression logistique utilise la fonction softmax :\n",
    "\n",
    "$$\n",
    "P(y = k | x) = \\frac{e^{\\beta_k^T x}}{\\sum_{j=1}^{K} e^{\\beta_j^T x}}\n",
    "$$\n",
    "\n",
    "Cette fonction garantit que $\\sum_{k=1}^{K} P(y = k | x) = 1$.\n",
    "\n",
    "**Exemple d'application :** Classifier les espèces d'iris (setosa, versicolor, virginica) à partir des caractéristiques morphologiques.\n",
    "\n",
    "## Limites de la séparation linéaire\n",
    "---\n",
    "\n",
    "### Problème de non-linéarité\n",
    "\n",
    "Quand la séparation des classes n'est pas linéaire dans l'espace des features, la régression logistique atteint ses limites de performance.\n",
    "\n",
    "**Analogie :** C'est comme essayer de diviser une pizza aux formes irrégulières avec une règle droite - parfois il faut \"courber l'espace\" ou ajouter des dimensions.\n",
    "\n",
    "### Solutions : SVM et transformations\n",
    "\n",
    "Les **Support Vector Machines (SVM)** résolvent ce problème en projetant les données dans un espace de dimension supérieure via des \"kernels\", où un hyperplan séparateur devient possible.\n",
    "\n",
    "**Techniques alternatives :**\n",
    "- Transformation polynomial des features\n",
    "- Kernels RBF (Gaussian)\n",
    "- Réseaux de neurones\n",
    "\n",
    "## Pipeline de preprocessing\n",
    "---\n",
    "\n",
    "### Étapes essentielles\n",
    "\n",
    "**Imputation** (`SimpleImputer`) : remplacer les valeurs manquantes\n",
    "- Stratégie numérique : moyenne, médiane\n",
    "- Stratégie catégorielle : mode, constante\n",
    "\n",
    "**Encodage** (`OneHotEncoder`) : transformer les variables catégorielles en indicateurs binaires\n",
    "\n",
    "**Standardisation** (`StandardScaler`) : centrer et réduire les variables\n",
    "$$\n",
    "x_{scaled} = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "### Importance de l'automatisation\n",
    "\n",
    "**Sans pipeline :** Risk de data leakage et erreurs manuelles\n",
    "**Avec pipeline :** Enchaînement automatisé et reproductible\n",
    "\n",
    "**Exemple concret :** Pour le dataset \"Country\", l'encodage protège contre les erreurs de prédiction sur des pays jamais vus dans le jeu de test.\n",
    "\n",
    "## Métriques d'évaluation\n",
    "---\n",
    "\n",
    "### Matrice de confusion\n",
    "\n",
    "Tableau croisé des prédictions vs vraies classes :\n",
    "\n",
    "|                | Prédit Négatif | Prédit Positif |\n",
    "|----------------|----------------|----------------|\n",
    "| **Vrai Négatif**   | TN             | FP             |\n",
    "| **Vrai Positif**   | FN             | TP             |\n",
    "\n",
    "**Analogie :** Un bulletin de notes croisant les objectifs du professeur avec les réalisations de l'étudiant.\n",
    "\n",
    "### Métriques fondamentales\n",
    "\n",
    "**Accuracy (exactitude) :**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Precision (précision) :**\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "Minimise les faux positifs.\n",
    "\n",
    "**Recall (rappel) :**\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "Minimise les faux négatifs.\n",
    "\n",
    "**F1-Score :**\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "Moyenne harmonique équilibrant precision et recall.\n",
    "\n",
    "### Choix contextuel des métriques\n",
    "\n",
    "**Détection COVID :** Privilégier le recall (ne manquer aucun cas positif)\n",
    "**Filtrage spam :** Privilégier la precision (éviter de bloquer des emails importants)\n",
    "**Données équilibrées :** L'accuracy peut suffire\n",
    "**Données déséquilibrées :** F1-score indispensable\n",
    "\n",
    "**Piège classique :** Prédire toujours \"1\" maximise le recall mais annule la precision. D'où l'importance du F1-score pour équilibrer.\n",
    "\n",
    "## Analyse ROC et AUC\n",
    "---\n",
    "\n",
    "### Courbe ROC\n",
    "\n",
    "La courbe ROC (Receiver Operating Characteristic) trace le **True Positive Rate** vs **False Positive Rate** pour tous les seuils de décision :\n",
    "\n",
    "- **TPR = Recall** = TP/(TP+FN)\n",
    "- **FPR** = FP/(FP+TN)\n",
    "\n",
    "### Métrique AUC\n",
    "\n",
    "L'**AUC (Area Under Curve)** synthétise la performance globale :\n",
    "- **AUC = 0.5** : modèle aléatoire\n",
    "- **AUC = 1.0** : modèle parfait\n",
    "- **AUC > 0.7** : performance généralement acceptable\n",
    "\n",
    "**Exemple médical :** Pour évaluer un test de diagnostic, la courbe ROC révèle la qualité discriminante à tous les seuils de décision possibles.\n",
    "\n",
    "## Applications pratiques et workflow\n",
    "---\n",
    "\n",
    "### Construction méthodique d'un modèle\n",
    "\n",
    "1. **Import et exploration** des données (pandas, scikit-learn)\n",
    "2. **Preprocessing** : gestion des valeurs manquantes, encodage, standardisation\n",
    "3. **Split stratifié** train/test\n",
    "4. **Pipeline automatisé** pour éviter les fuites de données\n",
    "5. **Entraînement** du modèle\n",
    "6. **Évaluation multi-critères** : matrice de confusion, métriques, ROC\n",
    "\n",
    "### Modélisation multi-classe\n",
    "\n",
    "- Utilisation de `np.argmax()` pour identifier la classe prédite\n",
    "- Analyse des probabilités par classe\n",
    "- Focus sur les observations à faible confiance\n",
    "\n",
    "## Bonnes pratiques et recommandations\n",
    "---\n",
    "\n",
    "### Surveillance des métriques\n",
    "\n",
    "- **Analyser recall vs precision** selon l'acceptabilité des erreurs\n",
    "- **Adapter class_weight** pour gérer le déséquilibre sans sur-corriger\n",
    "- **Utiliser les pipelines** pour fiabiliser le processus\n",
    "\n",
    "### Workflow de validation\n",
    "\n",
    "1. **Comprendre la fonction de coût** et son impact sur l'apprentissage\n",
    "2. **Choisir les métriques** selon le contexte métier\n",
    "3. **Stratifier les splits** pour préserver les proportions\n",
    "4. **Vérifier la généralisation** via les scores de test\n",
    "5. **Ajuster les seuils** si nécessaire pour optimiser FP vs FN\n",
    "\n",
    "### Perspectives d'amélioration\n",
    "\n",
    "- **Optimisation des seuils** (abordée en détail dans le module M06D05)\n",
    "- **Modèles ensemble** pour améliorer la robustesse\n",
    "- **Techniques de rééchantillonnage** pour les déséquilibres sévères\n",
    "\n",
    "Cette base théorique et pratique de la classification prépare l'exploration des techniques avancées d'optimisation et des modèles d'arbres décisionnels présentés dans les modules suivants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5d056",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
