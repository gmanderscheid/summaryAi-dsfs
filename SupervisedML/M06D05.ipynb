{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05326790",
   "metadata": {},
   "source": [
    "# Classification Avancée et Optimisation des Modèles d'Arbres\n",
    "---\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/SupervisedML/M06D05_DataScience.m4a\n",
    "\n",
    "## Introduction : Vers une classification robuste et explicable\n",
    "---\n",
    "\n",
    "Ce module synthétise trois piliers essentiels de la classification avancée en machine learning : le **compromis biais-variance** qui gouverne la capacité de généralisation, l'**évaluation fine** via des métriques adaptées aux contraintes métier, et l'**optimisation des modèles d'arbres** pour allier performance et interprétabilité.\n",
    "\n",
    "Ces concepts forment l'architecture conceptuelle de tout projet de classification performant, particulièrement dans des contextes où les données sont déséquilibrées, les coûts d'erreur asymétriques, ou l'explicabilité réglementairement requise.\n",
    "\n",
    "## Le compromis biais-variance : Fondement théorique de la généralisation\n",
    "---\n",
    "\n",
    "### Définition et décomposition de l'erreur\n",
    "\n",
    "Le **compromis biais-variance** constitue le principe fondamental régissant la performance des modèles d'apprentissage automatique. L'erreur totale d'un modèle se décompose en :\n",
    "\n",
    "$$\n",
    "\\text{Erreur totale} = \\text{Biais}^2 + \\text{Variance} + \\text{Bruit irréductible}\n",
    "$$\n",
    "\n",
    "**Le biais** mesure l'erreur systématique due aux hypothèses simplificatrices du modèle, menant au **sous-apprentissage (underfitting)**.\n",
    "\n",
    "**La variance** quantifie la sensibilité du modèle aux variations des données d'entraînement, pouvant causer un **surapprentissage (overfitting)**.\n",
    "\n",
    "### Analogie de l'archer\n",
    "\n",
    "**Archer à fort biais :** Tire consistamment au même endroit, loin du centre. Ses tirs sont **cohérents mais imprécis** (sous-apprentissage).\n",
    "\n",
    "**Archer à forte variance :** Ses tirs se dispersent largement sur la cible sans cohérence. **Manque de stabilité** (surapprentissage).\n",
    "\n",
    "**Archer optimal :** Combine faible biais et faible variance pour atteindre régulièrement le centre avec précision et constance.\n",
    "\n",
    "### Profils algorithmiques biais-variance\n",
    "\n",
    "| Algorithme | Biais | Variance | Caractéristiques |\n",
    "|------------|--------|----------|------------------|\n",
    "| **Régression linéaire** | Élevé | Faible | Simple, stable, peut sous-ajuster |\n",
    "| **Arbres de décision** | Faible | Élevée | Flexibles, instables, sur-ajustent |\n",
    "| **Random Forest** | Faible | Réduite | Équilibre via agrégation |\n",
    "| **SVM linéaire** | Élevé | Faible | Robuste, peut manquer complexité |\n",
    "| **k-NN (k faible)** | Faible | Élevée | Très flexible, sensible au bruit |\n",
    "\n",
    "**Exemple concret :** En détection de fraude bancaire, un modèle linéaire simple (fort biais) pourrait manquer les patterns sophistiqués de fraude, tandis qu'un arbre très profond (forte variance) risque de mémoriser des anomalies non-représentatives.\n",
    "\n",
    "## Métriques d'évaluation avancées : Au-delà de l'accuracy\n",
    "---\n",
    "\n",
    "### Limites critiques de l'accuracy\n",
    "\n",
    "L'accuracy peut être **trompeuse en contexte déséquilibré**. Avec 99% de transactions légitimes et 1% frauduleuses, un modèle prédisant systématiquement \"légitime\" atteint 99% d'accuracy **sans détecter aucune fraude**.\n",
    "\n",
    "### Métriques fondamentales revisitées\n",
    "\n",
    "**Accuracy (exactitude globale) :**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Precision (précision) - Minimise les FP :**\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Recall (rappel) - Minimise les FN :**\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**F1-Score (moyenne harmonique) :**\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "### Stratégie de sélection contextuelle\n",
    "\n",
    "**Médecine (diagnostic cancer) :** Privilégier le **recall** pour ne manquer aucun cas positif (minimiser FN)\n",
    "\n",
    "**Filtrage spam :** Favoriser la **precision** pour éviter de bloquer des emails importants (minimiser FP)\n",
    "\n",
    "**E-commerce (recommandations) :** Équilibrer avec le **F1-score** pour optimiser satisfaction client\n",
    "\n",
    "**Systèmes critiques :** Utiliser des **métriques coût-sensibles** reflétant l'impact business réel\n",
    "\n",
    "## Optimisation du seuil de décision : Réglage fin des performances\n",
    "---\n",
    "\n",
    "### Au-delà du seuil par défaut 0.5\n",
    "\n",
    "La plupart des modèles utilisent un **seuil de 0.5 par défaut**, souvent inadapté aux données déséquilibrées ou aux contraintes métier. L'optimisation du seuil constitue une stratégie puissante **sans ré-entraînement**.\n",
    "\n",
    "**Principe :** Ajuster le point de décision pour transformer les probabilités en prédictions binaires selon les priorités métier.\n",
    "\n",
    "### Impact du seuil sur les métriques\n",
    "\n",
    "**Seuil élevé (ex: 0.8) :** Le modèle devient plus \"exigeant\"\n",
    "- ↗️ **Precision** (moins de faux positifs)\n",
    "- ↘️ **Recall** (plus de faux négatifs)\n",
    "\n",
    "**Seuil faible (ex: 0.2) :** Le modèle devient plus \"sensible\"\n",
    "- ↘️ **Precision** (plus de faux positifs)\n",
    "- ↗️ **Recall** (moins de faux négatifs)\n",
    "\n",
    "### Techniques d'optimisation automatisée\n",
    "\n",
    "**GHOST (Generic Hyperparameter Optimization for Stochastic Thresholding) :** Méthode automatisant l'optimisation du seuil pour différents classifieurs, souvent plus efficace que les techniques de rééchantillonnage traditionnelles sur données déséquilibrées.\n",
    "\n",
    "**Validation par courbe PR :** La courbe Precision-Recall guide le choix optimal du seuil selon les priorités métier.\n",
    "\n",
    "## Arbres de décision : Transparence et performance\n",
    "---\n",
    "\n",
    "### Principe de construction et pureté\n",
    "\n",
    "Les arbres de décision construisent des **règles séquentielles** pour maximiser la \"pureté\" des sous-groupes formés à chaque division. Le processus itératif sélectionne la variable et le seuil optimisant un critère de pureté.\n",
    "\n",
    "### Score de Gini : mesure d'impureté\n",
    "\n",
    "Le **score de Gini** quantifie l'hétérogénéité d'un nœud :\n",
    "\n",
    "$$\n",
    "\\text{Gini} = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "\n",
    "où $p_i$ représente la proportion de la classe $i$ dans le nœud.\n",
    "\n",
    "**Interprétation :**\n",
    "- **Gini = 0** : nœud parfaitement pur (une seule classe)\n",
    "- **Gini = 0.5** : impureté maximale (distribution uniforme binaire)\n",
    "\n",
    "**Exemple de calcul :** Un nœud contenant 60% de classe A et 40% de classe B :\n",
    "$$\n",
    "\\text{Gini} = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48\n",
    "$$\n",
    "\n",
    "### Gain d'information et sélection des splits\n",
    "\n",
    "Le **gain de Gini** mesure l'amélioration apportée par une division :\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\text{Gini}_{parent} - \\sum_{j} \\frac{n_j}{n} \\times \\text{Gini}_{enfant_j}\n",
    "$$\n",
    "\n",
    "L'algorithme sélectionne la division maximisant ce gain.\n",
    "\n",
    "### Interprétabilité et règles de décision\n",
    "\n",
    "Les arbres permettent une **interprétation naturelle** sous forme de règles \"SI-ALORS\" :\n",
    "\n",
    "- \"SI âge < 30 ET revenu > 50k€ ALORS risque_crédit = faible\"\n",
    "- \"SI historique_paiement = 'mauvais' ALORS risque_crédit = élevé\"\n",
    "\n",
    "Cette **transparence décisionnelle** est cruciale pour les domaines réglementés (finance, médecine, justice) nécessitant une explicabilité complète.\n",
    "\n",
    "## Random Forest : Puissance collective et robustesse\n",
    "---\n",
    "\n",
    "### Principe du bagging et diversité\n",
    "\n",
    "Les **Random Forests** agrègent de multiples arbres entraînés sur des sous-ensembles différents des données (**bootstrapping**) avec sélection aléatoire des variables à chaque division.\n",
    "\n",
    "**Mécanisme de réduction de variance :**\n",
    "$$\n",
    "\\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} \\text{ (si arbres indépendants)}\n",
    "$$\n",
    "\n",
    "En pratique, la corrélation entre arbres limite cette réduction, d'où l'importance de la diversification.\n",
    "\n",
    "### Analogie du panel d'experts\n",
    "\n",
    "Plutôt que consulter un seul expert (arbre unique) susceptible d'erreurs individuelles, on consulte un **panel de spécialistes** (forêt) pour une décision plus robuste et moins sujette aux biais particuliers.\n",
    "\n",
    "### Stratégies anti-surapprentissage\n",
    "\n",
    "Les Random Forests combattent l'overfitting par plusieurs mécanismes synergiques :\n",
    "\n",
    "**Diversité des modèles :** Chaque arbre observe des données différentes (bootstrap)\n",
    "\n",
    "**Sélection aléatoire des features :** Évite la dominance des variables les plus prédictives\n",
    "\n",
    "**Contrôle de la profondeur :** Limite la complexité individuelle des arbres\n",
    "\n",
    "**Agrégation :** Lisse les prédictions extrêmes par vote majoritaire (classification) ou moyenne (régression)\n",
    "\n",
    "## Importance des variables : Décryptage des facteurs clés\n",
    "---\n",
    "\n",
    "### Calcul de l'importance dans Random Forest\n",
    "\n",
    "L'**importance des variables** révèle la contribution de chaque feature aux prédictions. Random Forest calcule cette importance en mesurant la **réduction d'impureté** apportée par chaque variable à travers tous les arbres :\n",
    "\n",
    "$$\n",
    "\\text{Importance}(X_j) = \\frac{1}{n_{trees}} \\sum_{t=1}^{n_{trees}} \\sum_{nodes} w_{node} \\times \\Delta\\text{Gini}_{X_j}\n",
    "$$\n",
    "\n",
    "où $w_{node}$ est la proportion d'échantillons atteignant le nœud.\n",
    "\n",
    "### Limitations et alternatives\n",
    "\n",
    "**Biais de l'importance Gini :** Favorise les variables numériques et catégorielles avec beaucoup de modalités.\n",
    "\n",
    "**Permutation Feature Importance :** Alternative plus robuste mesurant la dégradation de performance quand on \"mélange\" aléatoirement une variable :\n",
    "\n",
    "$$\n",
    "\\text{PFI}(X_j) = \\text{Score}_{original} - \\text{Score}_{X_j \\text{ permutée}}\n",
    "$$\n",
    "\n",
    "### Applications sectorielles\n",
    "\n",
    "**Finance :** Identifier les facteurs de risque crédit principaux (ratio dette/revenu, ancienneté emploi)\n",
    "\n",
    "**Médecine :** Déterminer les biomarqueurs les plus prédictifs d'une pathologie\n",
    "\n",
    "**Marketing :** Comprendre les leviers d'engagement client les plus efficaces\n",
    "\n",
    "**Ressources humaines :** Analyser les facteurs de rétention des talents\n",
    "\n",
    "## Études de cas et applications pratiques\n",
    "---\n",
    "\n",
    "### Détection de fraude bancaire\n",
    "\n",
    "**Défis spécifiques :**\n",
    "- Données extrêmement déséquilibrées (0.1% de fraudes)\n",
    "- Coût asymétrique des erreurs (FN >> FP en impact business)\n",
    "- Besoin d'explicabilité pour les enquêtes\n",
    "\n",
    "**Solutions techniques :**\n",
    "- Optimisation du seuil de décision\n",
    "- Métriques adaptées (PR-AUC > ROC-AUC)\n",
    "- Random Forest pour capturer patterns complexes\n",
    "- Feature importance pour l'analyse forensique\n",
    "\n",
    "### Diagnostic médical automatisé\n",
    "\n",
    "**Exigences réglementaires :**\n",
    "- Explicabilité maximale des décisions\n",
    "- Minimisation absolue des faux négatifs\n",
    "- Traçabilité des facteurs de décision\n",
    "\n",
    "**Approche méthodologique :**\n",
    "- Arbres de décision pour la transparence\n",
    "- Validation croisée stratifiée\n",
    "- Analyse d'importance pour identifier biomarqueurs\n",
    "- Seuils ajustés vers la sensibilité maximale\n",
    "\n",
    "### Systèmes de recommandation\n",
    "\n",
    "**Challenges techniques :**\n",
    "- Données multi-dimensionnelles et sparse\n",
    "- Préférences utilisateur évolutives\n",
    "- Trade-off précision/diversité\n",
    "\n",
    "**Solutions d'ensemble :**\n",
    "- Random Forest pour la robustesse\n",
    "- Optimisation multi-objectifs\n",
    "- Pipeline adaptatif aux nouveaux utilisateurs\n",
    "\n",
    "## Synthèse et recommandations stratégiques\n",
    "---\n",
    "\n",
    "### Principes directeurs\n",
    "\n",
    "**Adaptation contextuelle :** Toujours aligner métriques et techniques sur les coûts métier réels\n",
    "\n",
    "**Équilibre performance-interprétabilité :** Choisir entre arbres simples (explicables) et Random Forest (performants) selon les contraintes\n",
    "\n",
    "**Validation rigoureuse :** Utiliser la validation croisée imbriquée pour des estimations non-biaisées\n",
    "\n",
    "**Pipeline modulaire :** Automatiser la chaîne complète tout en conservant la flexibilité\n",
    "\n",
    "### Workflow de développement recommandé\n",
    "\n",
    "1. **Analyse exploratoire :** Distribution des classes, qualité des données, patterns métier\n",
    "2. **Baseline robuste :** Établir une référence avec modèles simples\n",
    "3. **Optimisation progressive :** Pipeline → Hyperparamètres → Seuils → Métriques\n",
    "4. **Validation multi-niveaux :** Validation croisée + holdout final\n",
    "5. **Déploiement reproductible :** Pipeline scikit-learn + monitoring continu\n",
    "\n",
    "### Perspectives d'évolution\n",
    "\n",
    "La maîtrise de ces concepts ouvre la voie vers :\n",
    "- **Modèles ensemble avancés** (Gradient Boosting, Stacking)\n",
    "- **Apprentissage automatique de features** (Deep Learning)\n",
    "- **Optimisation automatique** (AutoML)\n",
    "- **Apprentissage en ligne** pour données en flux continu\n",
    "\n",
    "Cette approche systémique de la classification avancée forme le socle technique pour aborder les défis complexes de l'IA en production, où robustesse, explicabilité et performance doivent converger vers des solutions durables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffd71b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
