{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4a6c2d",
   "metadata": {},
   "source": [
    "# Méthodes d'Ensemble en Machine Learning \n",
    "\n",
    "---\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/SupervisedML/M06D06_DataScience.m4a\n",
    "\n",
    "## Introduction aux méthodes ensemblistes\n",
    "\n",
    "Les méthodes d'ensemble constituent une approche puissante pour améliorer la précision et la robustesse des prédictions en machine learning. Plutôt que de s'appuyer sur un seul modèle, elles combinent plusieurs algorithmes pour capitaliser sur la diversité et réduire la variance, tout en gardant un biais maîtrisé. Ce principe s'appuie sur la sagesse collective, reflet des modèles simples et avancés explorés dans les modules précédents.\n",
    "\n",
    "---\n",
    "\n",
    "## Comprendre le surapprentissage des arbres de décision\n",
    "\n",
    "Un arbre de décision complexe peut mémoriser les données d'entraînement, ce qui se traduit par une forte variance et une faible généralisation. Pour limiter ce surapprentissage, plusieurs hyperparamètres clés sont utilisés :  \n",
    "- **max_depth** : Limite la profondeur de l'arbre  \n",
    "- **min_samples_leaf** : Échantillons minimaux par feuille  \n",
    "- **min_samples_split** : Échantillons minimaux pour effectuer une division\n",
    "\n",
    "---\n",
    "\n",
    "## Bagging et Bootstrap Aggregating\n",
    "\n",
    "Le bootstrap consiste à générer plusieurs sous-échantillons d'un dataset par tirage aléatoire avec remise.\n",
    "\n",
    "### Processus :\n",
    "1. Sélection aléatoire avec remise\n",
    "2. Taille des sous-datasets identique à l'original\n",
    "3. Création de bootstrap datasets\n",
    "\n",
    "**Formule de variance réduite par bagging** :\n",
    "$$\n",
    "\\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{N}\n",
    "$$\n",
    "\n",
    "Le bagging (Bootstrap Aggregating) combine ces sous-échantillonnages pour entraîner plusieurs modèles similaires (par exemple, des arbres de décision) dont les prédictions sont ensuite agrégées par vote majoritaire (classification) ou moyenne (régression).\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest : Extension du Bagging\n",
    "\n",
    "Les Random Forests ajoutent au bagging une sélection aléatoire des variables à chaque division de nœud, accroissant la diversité des modèles.\n",
    "\n",
    "### Paramètres :\n",
    "- **n_estimators** : Nombre d'arbres\n",
    "- Même hyperparamètres que les arbres individuels\n",
    "\n",
    "---\n",
    "\n",
    "## AdaBoost : Apprentissage Adaptatif Séquentiel\n",
    "\n",
    "L'algorithme AdaBoost convertit les labels en {-1, +1} et attribue à chaque observation un poids initial uniforme. À chaque itération, le modèle apprend de nouvelles erreurs pondérées et ajuste les poids pour se focaliser sur les cas difficiles.\n",
    "\n",
    "### Erreur pondérée :\n",
    "$$\n",
    "\\epsilon_t = \\sum_{i=1}^{n} D_t(i) \\cdot \\mathbf{1}[h_t(x_i) \\neq y_i]\n",
    "$$\n",
    "\n",
    "### Poids du modèle :\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)\n",
    "$$\n",
    "\n",
    "### Mise à jour des poids d'observations :\n",
    "$$\n",
    "D_{t+1}(i) = \\frac{D_t(i) \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot h_t(x_i))}{Z_t}\n",
    "$$\n",
    "\n",
    "### Prédiction finale de l'ensemble :\n",
    "$$\n",
    "H(x) = \\text{sign}\\left(\\sum_{t=1}^{T} \\alpha_t \\cdot h_t(x)\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Boosting et XGBoost\n",
    "\n",
    "Contrairement à AdaBoost qui pondère les observations, le Gradient Boosting entraîne chaque nouveau modèle à prédire les erreurs (résidus) du modèle précédent. Cette approche utilise la descente de gradient pour minimiser une fonction de perte.\n",
    "\n",
    "### Formulation mathématique :\n",
    "$$\n",
    "F_0(x) = \\arg\\min_c \\sum_{i=1}^n L(y_i, c)\n",
    "$$\n",
    "\n",
    "À chaque itération $m$, le modèle de base prédit les pseudo-résidus :\n",
    "$$\n",
    "r_{i,m} = -\\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}\n",
    "$$\n",
    "\n",
    "Le nouveau prédicteur $h_m$ est ajusté sur ces résidus, puis :\n",
    "$$\n",
    "F_{m}(x) = F_{m-1}(x) + \\nu h_m(x)\n",
    "$$\n",
    "où $\\nu$ est le learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Voting Classifier : Démocratie des Modèles\n",
    "\n",
    "Plusieurs modèles différents votent pour chaque prédiction, et la classe majoritaire l'emporte. Contrairement au Random Forest qui repose sur le même type de base learner, le voting permet de combiner des algorithmes hétérogènes.\n",
    "\n",
    "### Formules de Voting :\n",
    "\n",
    "**Hard Voting :**\n",
    "$$\n",
    "\\hat{y} = \\text{mode}\\{h_1(x), h_2(x), ..., h_k(x)\\}\n",
    "$$\n",
    "\n",
    "**Soft Voting :**\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c \\left[\\frac{1}{k}\\sum_{i=1}^k p_{i,c}(x)\\right]\n",
    "$$\n",
    "\n",
    "**Weighted Voting :**\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c \\left[\\sum_{i=1}^k w_i \\cdot p_{i,c}(x)\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Stacking : Méta-apprentissage Hiérarchique\n",
    "\n",
    "En stacking, les prédictions des modèles de niveau 0 deviennent les nouvelles features d'entrée pour un méta-modèle de niveau 1 qui apprend à combiner ces résultats.\n",
    "\n",
    "**Processus :**\n",
    "1. Les modèles de base sont entraînés sur les données originales\n",
    "2. Leurs prédictions sont utilisées en tant que nouvelles features\n",
    "3. Le méta-modèle apprend sur ce nouvel espace pour optimiser la prédiction finale\n",
    "\n",
    "---\n",
    "\n",
    "## Applications pratiques et conseils d'optimisation\n",
    "\n",
    "- Chaque ensemble doit être adapté au problème : robustesse (Random Forest), performance maximale (XGBoost), efficacité computationnelle (AdaBoost), interprétabilité (Voting)\n",
    "- Sur classes déséquilibrées, privilégier le F1-score et pondérer les classes\n",
    "- Validation croisée imbriquée recommandée pour éviter le surapprentissage lors de l'optimisation des hyperparamètres\n",
    "- Sauvegarder les résultats pour suivis en production, via fichiers CSV et processus reproductibles\n",
    "\n",
    "---\n",
    "\n",
    "## Bonnes pratiques et perspectives stratégiques\n",
    "\n",
    "- Comparer la performance de chaque ensemble par rapport à une baseline simple\n",
    "- Favoriser la diversité des modèles pour maximiser la robustesse\n",
    "- Documenter et analyser systématiquement l'impact des hyperparamètres\n",
    "- S'assurer que la structure des ensembles est cohérente avec l'approche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c679e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
