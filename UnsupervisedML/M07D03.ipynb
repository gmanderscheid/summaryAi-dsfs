{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5629fd16",
   "metadata": {},
   "source": [
    "# Machine Learning : Analyse en Composantes Principales (PCA)\n",
    "\n",
    "Ce cours de 3 heures représente une plongée approfondie dans les **fondements mathématiques** de l'Analyse en Composantes Principales, technique fondamentale de réduction de dimensionnalité. L'approche pédagogique privilégie la compréhension des concepts sous-jacents plutôt que l'utilisation aveugle des outils.\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/UnSupervisedML/M07D03_DataScience.m4a\n",
    "\n",
    "## **Introduction et Contexte Pédagogique**\n",
    "\n",
    "### **Transition Naturelle du Clustering vers la PCA**\n",
    "\n",
    "Le cours débute par une **révision synthétique** des algorithmes de clustering vus précédemment :[1]\n",
    "\n",
    "**K-means** : \"Algorithme qui fait des **moyennes**\" lors du recalcul des centroïdes. Le processus itératif (initialisation → attribution → recalcul → convergence) converge vers des partitions sphériques optimales selon le critère WCSS.\n",
    "\n",
    "**DBSCAN** : \"Clustering basé sur la **densité**\" avec ses paramètres critiques ε (epsilon) et min_samples. Capable de gérer des formes arbitraires contrairement à K-means.\n",
    "\n",
    "Cette révision établit un **continuum pédagogique** : du clustering (grouper des observations similaires) vers la PCA (identifier les directions de variance maximale).\n",
    "\n",
    "### **Métriques d'Évaluation Rappelées**\n",
    "\n",
    "**WCSS (Within Cluster Sum of Squares)** : Mesure la compacité intra-cluster. **Propriété fondamentale** : décroît mécaniquement avec l'augmentation de K, d'où la nécessité d'utiliser la méthode du coude.[1]\n",
    "\n",
    "**Silhouette Score** : Évalue la **séparation inter-clusters** :[1]\n",
    "$$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "\n",
    "L'interprétation des **points en bordure** (silhouette négative) illustre la complexité des données réelles où les clusters peuvent se chevaucher.\n",
    "\n",
    "## **Fondements Mathématiques de la PCA**\n",
    "\n",
    "### **Problématique de la Covariance**\n",
    "\n",
    "Le cours introduit la PCA par l'observation d'un **nuage de points corrélés**. Quand les variables X et Y varient ensemble, leur covariance est non-nulle, créant une **matrice de covariance non-diagonale** :[1]\n",
    "\n",
    "$$\n",
    "\\text{Cov} = \\begin{pmatrix} 1 & 0.6 \\\\ 0.6 & 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Objectif de la PCA** : Transformer cette matrice en forme **diagonale** :\n",
    "$$\n",
    "\\text{Cov}_{diag} = \\begin{pmatrix} 4.2 & 0 \\\\ 0 & 2.1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Cette transformation garantit l'**indépendance** des nouvelles variables (composantes principales).\n",
    "\n",
    "### **Variance et Covariance : Définitions Rigoureuses**\n",
    "\n",
    "**Variance** :[2][1]\n",
    "$$ \\text{Var}(X) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n} $$\n",
    "\n",
    "**Covariance** :[2][1]\n",
    "$$ \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n} $$\n",
    "\n",
    "L'exemple pédagogique montre comment interpréter une covariance de 0.6 : \"Quand X augmente, Y a tendance à augmenter aussi, mais pas parfaitement\".[1]\n",
    "\n",
    "### **Matrice de Covariance : Propriétés Essentielles**\n",
    "\n",
    "**Symétrie** : Cov(X,Y) = Cov(Y,X) - la covariance est commutative.[2]\n",
    "\n",
    "**Semi-définie positive** : Toutes les valeurs propres sont ≥ 0, garantissant que les variances ne peuvent être négatives.[2]\n",
    "\n",
    "**Diagonalisabilité** : Les matrices de covariance sont **toujours diagonalisables** car symétriques - propriété cruciale qui rend la PCA possible.[1]\n",
    "\n",
    "## **Diagonalisation et Vecteurs Propres**\n",
    "\n",
    "### **L'Équation Fondamentale**\n",
    "\n",
    "La formule centrale **AX = λX** est introduite avec une explication géométrique intuitive : \"On cherche des vecteurs qui, quand on leur applique la matrice A, ne font que s'allonger, rétrécir ou s'inverser **sans rotation**\".[3][1]\n",
    "\n",
    "Cette approche évite l'abstraction mathématique pure en donnant une **signification visuelle** aux vecteurs propres.\n",
    "\n",
    "### **Interprétation Géométrique Avancée**\n",
    "\n",
    "**Vecteurs propres** : Directions privilégiées dans l'espace des données. Sur l'exemple du cours, le vecteur propre principal suit la **direction de variance maximale** du nuage de points.[1]\n",
    "\n",
    "**Valeurs propres** : Quantifient l'importance de chaque direction :[4]\n",
    "- **λ > 1** : Amplification de la variance dans cette direction\n",
    "- **0 < λ < 1** : Réduction de la variance\n",
    "- **λ = 0** : Direction sans variance (compression totale)\n",
    "\n",
    "### **Processus de Calcul Détaillé**\n",
    "\n",
    "Le cours détaille le **calcul manuel** pour une matrice 2×2 :[1]\n",
    "\n",
    "**Matrice exemple** : A = $$\\begin{pmatrix} 5 & 1 \\\\ 2 & 3 \\end{pmatrix}$$\n",
    "\n",
    "**Équation caractéristique** : $$\\det(A - \\lambda I) = 0$$\n",
    "\n",
    "**Développement** : $$\\lambda^2 - 8\\lambda + 11 = 0$$\n",
    "\n",
    "**Solution par discriminant** :[1]\n",
    "$$ \\Delta = 64 - 44 = 20 $$\n",
    "$$ \\lambda_{1,2} = \\frac{8 \\pm \\sqrt{20}}{2} $$\n",
    "\n",
    "Résultats : λ₁ = 6.25, λ₂ = 1.75\n",
    "\n",
    "Cette approche **\"retour aux sources\"** permet de comprendre que les outils comme `np.linalg.eig()` résolvent le même système, mais pour des matrices de dimension arbitraire.\n",
    "\n",
    "## **Processus PCA Complet**\n",
    "\n",
    "### **Pipeline en 5 Étapes**\n",
    "\n",
    "**Étape 1 : Normalisation (Cruciale)**[1]\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "```\n",
    "**Justification** : La PCA est **extrêmement sensible** à l'échelle des variables. Sans normalisation, les variables à plus grande amplitude domineraient artificiellement.\n",
    "\n",
    "**Étape 2 : Matrice de Covariance**[1]\n",
    "```python\n",
    "cov_matrix = pd.DataFrame(X_normalized).cov()\n",
    "```\n",
    "\n",
    "**Étape 3 : Calcul des Vecteurs et Valeurs Propres**[1]\n",
    "```python\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "```\n",
    "\n",
    "**Étape 4 : Tri par Importance**\n",
    "Les valeurs propres déterminent l'ordre des composantes principales.\n",
    "\n",
    "**Étape 5 : Projection**[1]\n",
    "```python\n",
    "X_pca = X_normalized.T.dot(eigenvectors)\n",
    "```\n",
    "\n",
    "### **Variance Expliquée : Interprétation Cruciale**\n",
    "\n",
    "**Formule du ratio** :[5][6]\n",
    "$$ \\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^n \\lambda_j} $$\n",
    "\n",
    "**Exemple concret du cours** :[1]\n",
    "- PC1 : λ₁ = 4.2 → **67% de variance**\n",
    "- PC2 : λ₂ = 2.1 → **33% de variance**\n",
    "- **Somme** : 67% + 33% = **100%** (conservation totale)\n",
    "\n",
    "Cette propriété garantit qu'**aucune information n'est perdue** dans la transformation, seulement **réorganisée** selon l'importance.\n",
    "\n",
    "## **Applications Pratiques et Limitations**\n",
    "\n",
    "### **Exemple Concret : Compression d'Images**\n",
    "\n",
    "L'illustration du cours montre une image 150×150 pixels (22 500 dimensions) réduite à 15-20 composantes principales tout en **conservant 95% de l'information visuelle**.[1]\n",
    "\n",
    "**Calcul d'efficacité** :\n",
    "- **Original** : 22 500 valeurs\n",
    "- **PCA** : 20 composantes + coefficients\n",
    "- **Ratio de compression** : ~50:1 avec qualité préservée\n",
    "\n",
    "### **Cas d'Usage en Machine Learning**\n",
    "\n",
    "**Preprocessing pour clustering** : Réduire 150 dimensions → 15 accélère K-means **sans perte significative** de qualité de clustering.[1]\n",
    "\n",
    "**Visualisation haute dimension** : L'exemple montré (200D → 3D) ne conserve que **8.5% de variance** mais reste **interprétable** pour identifier des patterns grossiers.[1]\n",
    "\n",
    "### **Limitations Critiques**\n",
    "\n",
    "**Hypothèse de linéarité** : La PCA assume des **relations linéaires** entre variables. Sur des données avec patterns non-linéaires, elle peut échouer.[1]\n",
    "\n",
    "**Interprétabilité des composantes** : Une composante principale combinant plusieurs variables originales peut devenir **difficile à interpréter** métier.\n",
    "\n",
    "**Perte d'information** : Même avec 95% de variance conservée, les **5% perdus** peuvent contenir l'information discriminante cruciale pour certaines tâches.\n",
    "\n",
    "## **Considérations Algorithmiques Avancées**\n",
    "\n",
    "### **SVD vs Eigen-décomposition**\n",
    "\n",
    "Le cours mentionne la **SVD (Singular Value Decomposition)** comme méthode alternative : A = UΣV^T. Cette approche est **numériquement plus stable** que l'eigen-décomposition directe, particulièrement pour de grandes matrices.[1]\n",
    "\n",
    "### **Choix du Nombre de Composantes**\n",
    "\n",
    "**Approche empirique recommandée** :[1]\n",
    "1. **Calculer PCA complète** (toutes les composantes)\n",
    "2. **Analyser la variance cumulée**\n",
    "3. **Choisir selon contexte** :\n",
    "   - **Visualisation** : 2-3 composantes\n",
    "   - **Machine Learning** : 90-95% de variance\n",
    "   - **Compression** : Trade-off qualité/taille\n",
    "\n",
    "### **Gestion des Signes**\n",
    "\n",
    "Point technique important : Les vecteurs propres peuvent avoir des **signes arbitraires** entre différentes exécutions. Cette **indétermination** n'affecte pas les résultats car seule la **direction** importe, pas l'orientation.[1]\n",
    "\n",
    "## **Intégration avec l'Écosystème ML**\n",
    "\n",
    "### **Relation avec le Clustering**\n",
    "\n",
    "**Message clé du cours** : La PCA et le clustering sont **complémentaires mais indépendants**. On peut :[1]\n",
    "- Faire du clustering **sans** PCA\n",
    "- Utiliser la PCA **avant** clustering pour accélération\n",
    "- Combiner selon les besoins spécifiques\n",
    "\n",
    "### **Workflow Recommandé**\n",
    "\n",
    "```python\n",
    "# Pipeline intégré\n",
    "def ml_pipeline_with_pca(X, target_variance=0.95):\n",
    "    # 1. Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 2. PCA avec seuil adaptatif\n",
    "    pca = PCA(n_components=target_variance)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # 3. Analyse de la réduction\n",
    "    print(f\"Réduction: {X.shape[1]} → {X_pca.shape[1]} dimensions\")\n",
    "    print(f\"Variance conservée: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    return X_pca, pca, scaler\n",
    "```\n",
    "\n",
    "## **Perspective Pédagogique et Ouverture**\n",
    "\n",
    "### **Progression Conceptuelle**\n",
    "\n",
    "Ce cours illustre parfaitement la **progression pédagogique** en data science :\n",
    "1. **Outils pratiques** (K-means, DBSCAN) → Applications directes\n",
    "2. **Fondements théoriques** (PCA) → Compréhension profonde\n",
    "3. **Intégration** → Vision systémique\n",
    "\n",
    "### **Préparation aux Concepts Avancés**\n",
    "\n",
    "La maîtrise des **vecteurs et valeurs propres** ouvre la voie vers :\n",
    "- **Méthodes spectrales** en clustering\n",
    "- **Analyse factorielle** \n",
    "- **Décompositions matricielles avancées** (NMF, ICA)\n",
    "- **Deep Learning** (auto-encoders)\n",
    "\n",
    "### **Ressources d'Approfondissement**\n",
    "\n",
    "Le cours recommande **3Blue1Brown** pour la visualisation géométrique des concepts d'algèbre linéaire - ressource incontournable pour développer l'intuition mathématique nécessaire.[1]\n",
    "\n",
    "Cette approche **\"bottom-up\"** (des calculs manuels aux outils sophistiqués) forge une compréhension robuste, essentielle pour une pratique éclairée du machine learning en environnement professionnel.\n",
    "\n",
    "[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/76884410/ee66b558-9ee1-4406-904e-9cc54cd861dd/transcriptM07D03.txt)\n",
    "[2](https://fr.wikipedia.org/wiki/Covariance)\n",
    "[3](https://ecampusontario.pressbooks.pub/equationsdifferentielles/chapter/6-3-revision-valeurs-propres-et-vecteurs-propres/)\n",
    "[4](https://www.youtube.com/watch?v=eT8DgrFQNqY)\n",
    "[5](https://vitalflux.com/pca-explained-variance-concept-python-example/)\n",
    "[6](https://www.geeksforgeeks.org/machine-learning/recovering-feature-names-of-explainedvarianceratio-in-pca-with-sklearn/)\n",
    "[7](https://www.datarockstars.ai/principal-component-analysis-pca/)\n",
    "[8](https://major-prepa.com/mathematiques/matrice-variance-covariance/)\n",
    "[9](https://dridk.me/analyse-en-composante-principale.html)\n",
    "[10](https://www.math.univ-paris13.fr/~tournier/fichiers/agreg/statistiques.pdf)\n",
    "[11](https://spss.espaceweb.usherbrooke.ca/analyse-en-composantes-principales-2/)\n",
    "[12](https://www.dcode.fr/vecteurs-propres-matrice)\n",
    "[13](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)\n",
    "[14](https://www.imo.universite-paris-saclay.fr/~bruno.duchesne/enseignement/complements_algebre_lineaire/vecteurs-propres-et-valeurs-propres.html)\n",
    "[15](http://www-lmpa.univ-littoral.fr/~smoch/documents/L2-Seg--ISCID-prepa2/diagonalisation.pdf)\n",
    "[16](https://www.youtube.com/watch?v=y5hzRYZxd4Y)\n",
    "[17](https://uel.unisciel.fr/physique/outils_nancy/outils_nancy_ch11/co/apprendre_ch11_20.html)\n",
    "[18](https://www.mathsetmaryam.fr/u/TE62MI-Analyse-en-composante-principale.pdf)\n",
    "[19](https://www.ibm.com/fr-fr/think/topics/principal-component-analysis)\n",
    "[20](http://exo7.emath.fr/cours/ch_vp.pdf)\n",
    "[21](https://lucidar.me/fr/mathematics/understanding-covariance-matrices/)\n",
    "[22](https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-explo-acp)\n",
    "[23](https://fr.wikipedia.org/wiki/Valeur_propre,_vecteur_propre_et_espace_propre)\n",
    "[24](https://support.ptc.com/help/mathcad/r10.0/fr/PTC_Mathcad_Help/singular_value_decomposition.html)\n",
    "[25](https://fr.wikipedia.org/wiki/Calcul_du_d%C3%A9terminant_d'une_matrice)\n",
    "[26](https://dev.to/rlrocha/choosing-the-number-of-components-of-principal-component-analysis-an-investigation-of-cumulative-explained-variance-ratio-251d)\n",
    "[27](https://www.mmm.uliege.be/files/MATH0013/syllabus/MATH0013_2222_SVD.pdf)\n",
    "[28](https://www.dcode.fr/determinant-matrice)\n",
    "[29](https://fr.wikipedia.org/wiki/D%C3%A9composition_en_valeurs_singuli%C3%A8res)\n",
    "[30](https://www.methodemaths.fr/determinant_matrice/)\n",
    "[31](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "[32](https://botafogo.saitis.net/algebre-lineaire/resources/pdfschapitres/chap-decomp-val-singulieres.pdf)\n",
    "[33](https://www.hec.ca/cams/rubriques-aide/notion-mathematiques/Les_determinants_des_matrices.pdf)\n",
    "[34](https://www.bibmath.net/dico/index.php?action=affiche&quoi=.%2Fv%2Fvalsing.html)\n",
    "[35](https://uel.unisciel.fr/physique/outils_nancy/outils_nancy_ch11/co/apprendre_ch11_15.html)\n",
    "[36](https://www.sciencedirect.com/topics/computer-science/variance-ratio)\n",
    "[37](https://perso.eleves.ens-rennes.fr/people/matthias.hostein/Fichiers_site/SVD_et_moindres_carres.pdf)\n",
    "[38](http://exo7.emath.fr/cours/ch_determinants.pdf)\n",
    "[39](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "[40](https://www.youtube.com/watch?v=5VDuVVPwh3w)\n",
    "[41](https://www.youtube.com/watch?v=_o6eMCpDgVc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f57e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
