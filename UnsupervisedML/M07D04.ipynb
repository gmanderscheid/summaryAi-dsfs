{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37265b12",
   "metadata": {},
   "source": [
    "# Natural Language Processing : Préprocessing de Texte et TF-IDF\n",
    "\n",
    "Ce cours de 3 heures représente une **transition fondamentale** de l'analyse de données numériques vers le traitement du langage naturel. Il illustre parfaitement la progression pédagogique : clustering de données structurées → réduction dimensionnelle → **traitement de données textuelles non-structurées**.\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/UnSupervisedML/M07D04_DataScience.mp3\n",
    "\n",
    "## **Introduction et Contextualisation Pédagogique**\n",
    "\n",
    "### **Révision Synthétique des Concepts Précédents**\n",
    "\n",
    "Le cours débute par une **récapitulation active** des algorithmes étudiés :[1]\n",
    "\n",
    "**K-means** : \"Algorithme qui fait des **moyennes**\" lors du recalcul des centroïdes. Processus itératif avec convergence vers WCSS minimale, mais attention au caractère **mécaniquement décroissant** de WCSS avec l'augmentation de K.\n",
    "\n",
    "**DBSCAN** : Clustering **basé sur la densité** avec paramètres critiques ε (epsilon) et min_samples. Capacité unique à **détecter des formes arbitraires** (croissants de lune) là où K-means échoue avec ses partitions sphériques.\n",
    "\n",
    "**PCA** : **Diagonalisation** de la matrice de covariance pour maximiser la variance expliquée. Transformation linéaire préservant l'information essentielle tout en réduisant la dimensionnalité.\n",
    "\n",
    "Cette révision établit une **continuité conceptuelle** : des données numériques bien structurées vers les données textuelles intrinsèquement non-structurées.\n",
    "\n",
    "### **Transition vers le NLP**\n",
    "\n",
    "**Problématique fondamentale** : Comment une machine peut-elle \"comprendre\" le texte alors qu'elle n'a aucune notion intrinsèque du sens des mots ?[1]\n",
    "\n",
    "**Réponse du cours** : Un mot n'est pas défini par ses lettres mais par **le contexte dans lequel il est habituellement observé**. Cette philosophie sous-tend toute la démarche NLP moderne.\n",
    "\n",
    "## **Fondements Théoriques du Natural Language Processing**\n",
    "\n",
    "### **Définition et Portée**\n",
    "\n",
    "Le NLP (Natural Language Processing) est défini comme **\"la discipline qui construit des machines capables de manipuler le langage humain\"**. Mais le cours va plus loin en précisant que le NLP traditionnel se concentre sur les **méthodes de preprocessing** plutôt que sur la compréhension sémantique profonde.[2][1]\n",
    "\n",
    "### **Distinction NLP vs NLU**\n",
    "\n",
    "**NLP** : Méthodes de transformation texte → vecteurs exploitables.[1]\n",
    "\n",
    "**NLU (Natural Language Understanding)** : Conversion audio → texte → vecteurs, incluant la reconnaissance vocale et l'identification des locuteurs.[1]\n",
    "\n",
    "**Exemple concret du professeur** : Analyser un transcript de cours pour identifier qui pose des questions (étudiant vs professeur) nécessite du NLU avec reconnaissance vocale, pas seulement du NLP textuel.[1]\n",
    "\n",
    "### **Défis Intrinsèques du Langage**\n",
    "\n",
    "**Polysémie contextuelle** : \"Apple\" signifie entreprise technologique ou fruit selon le contexte. Cette ambiguïté nécessite une analyse contextuelle sophistiquée.[1]\n",
    "\n",
    "**Variations culturelles** : Les concepts politiques s'expriment différemment selon les cultures (États-Unis vs Corée). Les embeddings doivent être **entraînés sur des données spécifiques à chaque langue et culture**.[1]\n",
    "\n",
    "**Subjectivité sémantique** : Le mot \"amour\" a des connotations différentes pour une personne de 20 ans vs 60 ans, illustrant que les humains eux-mêmes ne s'accordent pas sur le sens des mots.[1]\n",
    "\n",
    "## **Architecture de Traitement : Du Texte aux Vecteurs**\n",
    "\n",
    "### **Problématique de la Vectorisation**\n",
    "\n",
    "**Contrainte fondamentale** : Les modèles ML ne traitent que des vecteurs numériques. Le défi consiste à transformer des chaînes de caractères en représentations numériques préservant l'information sémantique.[1]\n",
    "\n",
    "### **Count Vectorizer : Première Approche**\n",
    "\n",
    "**Principe** : Matrice avec documents en lignes, mots uniques en colonnes, occurrences en valeurs.[1]\n",
    "\n",
    "**Exemple pratique du cours** :\n",
    "```\n",
    "Corpus:\n",
    "- \"Guillaume va à la plage\"  \n",
    "- \"Guillaume va à la salle\"\n",
    "- \"Guillaume est prof\"\n",
    "```\n",
    "\n",
    "**Matrice résultante** :\n",
    "```\n",
    "           Guillaume  va  à  la  plage  salle  est  prof\n",
    "Document1      1      1   1   1    1     0    0    0\n",
    "Document2      1      1   1   1    0     1    0    0\n",
    "Document3      1      0   0   0    0     0    1    1\n",
    "```\n",
    "\n",
    "### **Problème de Sparsité Critique**\n",
    "\n",
    "**Dimensionalité explosive** : Avec 10 000 documents et 10 000 mots uniques → 100 millions de cellules avec **~99,5% de zéros**.[1]\n",
    "\n",
    "**Calcul concret du cours** : 50 mots moyens par document sur 10 000 colonnes → **9 950 zéros par ligne** en moyenne.\n",
    "\n",
    "**Solution technique** : **Sparse Matrix** stockant uniquement les valeurs non-nulles sous forme de dictionnaire.[1]\n",
    "\n",
    "**Piège fréquent** : Conversion sparse matrix → DataFrame provoque des **erreurs mémoire** sur de gros datasets (150 millions de cellules = ~1.2 GB RAM minimum).\n",
    "\n",
    "## **Stratégies de Preprocessing Fondamentales**\n",
    "\n",
    "### **Règles de Filtrage Intelligentes**\n",
    "\n",
    "**Première règle** : Les mots **communs à tous les documents** ne permettent pas de différencier un document d'un autre → suppression.[1]\n",
    "\n",
    "**Deuxième règle** : Les mots **présents dans un seul document** n'apportent rien pour le clustering ou la classification → suppression également.[1]\n",
    "\n",
    "**Logique sous-jacente** : Conserver uniquement les mots ayant un **pouvoir discriminant** optimal.\n",
    "\n",
    "### **Stop Words : Filtrage Sophistiqué**\n",
    "\n",
    "**Définition contextuelle** : Mots très fréquents ne portant pas de sens discriminant dans le contexte spécifique.[1]\n",
    "\n",
    "**spaCy français** : **326 stop words** par défaut. Liste personnalisable selon le domaine :[3]\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_lg\") \n",
    "stop_words = nlp.Defaults.stop_words\n",
    "stop_words.add(\"guillaume\")  # Ajout contextuel\n",
    "```\n",
    "\n",
    "**Stratégie recommandée** : Analyser les mots les plus fréquents **après premier preprocessing**, identifier manuellement ceux à ajouter.[1]\n",
    "\n",
    "### **Lemmatisation vs Stemming**\n",
    "\n",
    "**Stemming** : Approche **\"brutale\"** utilisant des règles heuristiques.[4][1]\n",
    "- Rapide, faible coût computationnel\n",
    "- Peut produire des mots inexistants (\"caring\" → \"car\")\n",
    "- Adapté aux moteurs de recherche\n",
    "\n",
    "**Lemmatisation** : Approche **\"intelligente\"** avec analyse morphologique.[5][1]\n",
    "- Lente, coût computationnel élevé  \n",
    "- Produit des mots valides du dictionnaire\n",
    "- Adaptée à l'analyse sémantique fine\n",
    "\n",
    "**Recommandation du cours** : Lemmatisation **uniquement si le budget computationnel le permet**, sinon stemming suffisant pour beaucoup d'applications.[1]\n",
    "\n",
    "## **TF-IDF : Revolution du Scoring**\n",
    "\n",
    "### **Limitation Identifiée du Count Vectorizer**\n",
    "\n",
    "**Problème** : Le simple comptage ignore l'**importance relative** d'un mot dans le corpus global.[1]\n",
    "\n",
    "**Exemple concret** : \"Guillaume\" apparaît dans tous les documents → **haute fréquence locale** mais **faible pouvoir discriminant**.\n",
    "\n",
    "### **Formulation Mathématique Détaillée**\n",
    "\n",
    "**Term Frequency (TF)** :[6][1]\n",
    "$$ TF = \\frac{\\text{occurrences du terme dans le document}}{\\text{mots totaux du document}} $$\n",
    "\n",
    "**Inverse Document Frequency (IDF)** :[6][1]\n",
    "$$ IDF = \\log\\left(\\frac{\\text{documents totaux}}{\\text{documents contenant le terme}}\\right) $$\n",
    "\n",
    "**Score final TF-IDF** :[6][1]\n",
    "$$ \\text{TF-IDF} = TF \\times IDF $$\n",
    "\n",
    "### **Exemple Calculé du Cours**\n",
    "\n",
    "**Configuration** :[1]\n",
    "- 2 documents total\n",
    "- Mot \"four\" : 2 occurrences dans doc 1, absent du doc 2  \n",
    "- Document 1 : 8 mots total\n",
    "\n",
    "**Calculs étape par étape** :\n",
    "1. **TF** = 2/8 = **0,25**\n",
    "2. **IDF** = log(2/1) = log(2) ≈ **0,69**\n",
    "3. **TF-IDF** = 0,25 × 0,69 ≈ **0,17**\n",
    "\n",
    "### **Propriété d'Auto-Pénalisation**\n",
    "\n",
    "**Cas critique** : Mot présent dans tous les documents.[1]\n",
    "- IDF = log(n/n) = log(1) = **0**\n",
    "- TF-IDF = TF × 0 = **0**\n",
    "\n",
    "**Objectif atteint** : Pénalisation automatique des mots trop fréquents, valorisation des mots localement importants mais globalement rares.\n",
    "\n",
    "## **Techniques Avancées et Embeddings**\n",
    "\n",
    "### **spaCy : Analyse Grammaticale Contextuelle**\n",
    "\n",
    "**Fonctionnalités avancées** :[1]\n",
    "- Détection de **noms propres** vs noms communs\n",
    "- Identification des **compléments d'objet direct/indirect**\n",
    "- **Relations de dépendance** entre mots dans la phrase\n",
    "\n",
    "**Solution au problème de casse** : Identifier les noms propres **avant** conversion en minuscules, les marquer spécialement (ex: \"apple#PROPN\").\n",
    "\n",
    "### **Word Embeddings et Similarité**\n",
    "\n",
    "**Principe révolutionnaire** : Représentation vectorielle dense basée sur le **contexte d'usage** plutôt que sur la forme des mots.[1]\n",
    "\n",
    "**Similarité Cosinus** : Mesure l'angle entre vecteurs de mots :[1]\n",
    "- **Angle faible** → mots sémantiquement proches  \n",
    "- **90 degrés** → mots orthogonaux (pas de relation)\n",
    "- **Angle important** → mots opposés\n",
    "\n",
    "**Application pratique** : Grouper automatiquement \"chat\", \"chats\", \"félin\" par proximité vectorielle.\n",
    "\n",
    "### **Limites des Embeddings**\n",
    "\n",
    "**Dépendance aux données d'entraînement** : Embeddings américains ne fonctionnent pas sur du coréen à cause des structures linguistiques différentes.[1]\n",
    "\n",
    "**Exemple concret** : \"Trump\" et \"chat\" pourraient avoir une faible distance cosinus si le corpus d'entraînement contenait beaucoup de comparaisons \"Trump se comporte comme un chat\".\n",
    "\n",
    "## **N-grammes : Préservation du Contexte**\n",
    "\n",
    "### **Nécessité Contextuelle Critique**\n",
    "\n",
    "**Exemple paradigmatique du cours** :[1]\n",
    "- \"This guy is **dead**\" → sentiment **négatif** (unigramme)\n",
    "- \"This guy is **to die for**\" → sentiment **positif** (trigramme)\n",
    "\n",
    "**Problème de la lemmatisation** : \"die\" et \"dead\" convergent vers la même racine, perdant la distinction sémantique cruciale.\n",
    "\n",
    "**Solution** : Capturer les n-grammes pour préserver le contexte sémantique.\n",
    "\n",
    "### **Explosion Combinatoire**\n",
    "\n",
    "**Défi technique** : Inclure tous les n-grammes multiplie exponentiellement le nombre de colonnes.[1]\n",
    "\n",
    "**Stratégie de mitigation** recommandée :[1]\n",
    "1. Analyser séparément par **classe cible** (positif/négatif/neutre)\n",
    "2. Identifier les n-grammes **les plus discriminants**\n",
    "3. N'inclure que ceux ayant un **fort impact différentiel**\n",
    "\n",
    "**Exemple pratique** : Plutôt que d'inclure tous les trigrammes, analyser lesquels sont surreprésentés dans les sentiments positifs vs négatifs.\n",
    "\n",
    "## **Considérations Pratiques et Limitations**\n",
    "\n",
    "### **Trade-offs Computationnels**\n",
    "\n",
    "**Correction orthographique** : Très efficace mais **prohibitivement lente**. Recommandation : réserver à l'exploration initiale sur échantillons réduits.[1]\n",
    "\n",
    "**spaCy vs méthodes simples** : Objets NLP riches en attributs mais **lourds en mémoire**. Tester sur sous-échantillons avant application complète.[1]\n",
    "\n",
    "### **Gestion Mémoire Critique**\n",
    "\n",
    "**Règle d'or** : Ne **jamais** convertir directement sparse matrix → DataFrame sans vérifier la taille.[1]\n",
    "\n",
    "**Exemple critique** : 10 000 docs × 15 000 mots = 150 millions de cellules → **crash mémoire** garanti sur machine standard.\n",
    "\n",
    "### **Approche Itérative Recommandée**\n",
    "\n",
    "**Workflow standard** :[1]\n",
    "1. **Preprocessing initial** + analyse vocabulaire\n",
    "2. **Ajustement** basé sur les statistiques observées  \n",
    "3. **Re-preprocessing optimisé**\n",
    "4. **Validation** sur modèle ML\n",
    "\n",
    "**Philosophie** : \"Il n'y a pas de solution miracle\", tout dépend du contexte et des contraintes.\n",
    "\n",
    "## **Perspectives et Évolution vers les LLM**\n",
    "\n",
    "### **Positionnement des Techniques Traditionnelles**\n",
    "\n",
    "**Vision nuancée du cours** : Les LLM ne rendent **pas obsolètes** les techniques de preprocessing traditionnelles. Elles restent cruciales pour :[1]\n",
    "\n",
    "1. **Comprendre les fondements** des modèles modernes\n",
    "2. **Optimiser les coûts** (preprocessing + petit modèle vs gros LLM)\n",
    "3. **Contrôler la qualité** des données d'entrée\n",
    "\n",
    "### **Usage LLM Recommandé**\n",
    "\n",
    "**Approche pédagogique du professeur** : Utiliser les LLM pour **brainstorming** et **validation méthodologique** plutôt que remplacement complet.[1]\n",
    "\n",
    "**Exemple concret** : Avant de préparer le cours, utiliser un LLM pour explorer différentes manières d'expliquer les concepts, enrichir les analogies, sourcer les références.\n",
    "\n",
    "**Principe directeur** : Approche **proactive** plutôt que passive (ne pas juste copier-coller le code généré).\n",
    "\n",
    "## **Impact Pédagogique et Méthodologique**\n",
    "\n",
    "Ce cours illustre parfaitement une progression pédagogique en data science : partir de problèmes numériques bien définis (clustering) vers des défis plus complexes et ambigus (NLP). \n",
    "\n",
    "L'approche **\"compromis partout\"** reflète la réalité professionnelle où chaque choix technique implique des trade-offs entre vitesse, précision, mémoire et complexité.\n",
    "\n",
    "La philosophie **\"pas de solution miracle\"** prépare les étudiants à l'adaptabilité nécessaire en environnement professionnel, où les contraintes métier dictent les choix techniques plutôt que les préférences algorithmiques.\n",
    "\n",
    "L'intégration réfléchie des LLM dans le workflow pédagogique montre comment évoluer avec les outils modernes tout en conservant une compréhension fondamentale des principes sous-jacents.\n",
    "\n",
    "[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/76884410/49454eeb-fcd1-4b2d-aaa9-b6c7bb3474a8/transcriptM07D04.txt)\n",
    "[2](https://www.ibm.com/think/topics/natural-language-processing)\n",
    "[3](https://blog.octo.com/nettoyage-du-texte-en-nlp-moins-de-vocabulaire-moins-de-bruit)\n",
    "[4](https://seonorth.ca/fr/nlp/stemming-and-lemmatization/)\n",
    "[5](https://codefinity.com/fr/courses/v2/c68c1f2e-2c90-4d5d-8db9-1e97ca89d15e/026d736a-1860-4e3e-a915-b926ca2d9ed8/9f624800-92f4-4e6d-bb3d-40e002209bd5)\n",
    "[6](https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/)\n",
    "[7](https://www.lyzr.ai/glossaries/tokenization/)\n",
    "[8](https://www.deeplearning.ai/resources/natural-language-processing/)\n",
    "[9](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/)\n",
    "[10](https://neptune.ai/blog/tokenization-in-nlp)\n",
    "[11](https://www.qualtrics.com/experience-management/customer/natural-language-processing/)\n",
    "[12](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "[13](https://codefinity.com/blog/A-Comprehensive-Guide-to-Text-Preprocessing-with-NLTK)\n",
    "[14](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "[15](https://fr.wikipedia.org/wiki/TF-IDF)\n",
    "[16](https://www.geeksforgeeks.org/nlp/nlp-how-tokenizing-text-sentence-words-works/)\n",
    "[17](https://datascientest.com/en/natural-language-processing-definition-and-principles)\n",
    "[18](https://datascientest.com/tf-idf-intelligence-artificielle)\n",
    "[19](https://www.codecademy.com/learn/dsnlp-text-preprocessing/modules/nlp-text-preprocessing/cheatsheet)\n",
    "[20](https://www.openstudio.fr/metiers/intelligence-artificielle/natural-language-processing-nlp/)\n",
    "[21](https://cuik.io/blog/lexique-seo/tf-idf-term-frequency-inverse-document-frequency/)\n",
    "[22](https://www.kaggle.com/code/satishgunjal/tokenization-in-nlp)\n",
    "[23](https://aws.amazon.com/fr/what-is/nlp/)\n",
    "[24](https://programminghistorian.org/fr/lecons/analyse-de-documents-avec-tfidf)\n",
    "[25](https://milvus.io/ai-quick-reference/what-are-stop-words-in-nlp)\n",
    "[26](https://www.cambridgespark.com/blog/word-embeddings-in-python)\n",
    "[27](https://codesignal.com/learn/courses/text-data-preprocessing-in-python/lessons/demystifying-stop-words-in-natural-language-processing)\n",
    "[28](https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/)\n",
    "[29](https://fr.mathworks.com/discovery/stemming.html)\n",
    "[30](https://stackoverflow.com/questions/73568510/spacy-models-with-different-word2vec-embeddings-give-same-results)\n",
    "[31](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)\n",
    "[32](https://fr.eitca.org/artificial-intelligence/eitc-ai-dltf-deep-learning-with-tensorflow/tensorflow/processing-data/examination-review-processing-data/what-is-the-difference-between-lemmatization-and-stemming-in-text-processing/)\n",
    "[33](https://spacy.io/usage/embeddings-transformers)\n",
    "[34](https://gist.github.com/sebleier/554280)\n",
    "[35](https://www.ibm.com/fr-fr/think/topics/stemming-lemmatization)\n",
    "[36](https://ashutoshtripathi.com/2020/09/04/word2vec-and-semantic-similarity-using-spacy-nlp-spacy-series-part-7/)\n",
    "[37](https://github.com/stopwords-iso/stopwords-fr)\n",
    "[38](https://fr.linkedin.com/advice/1/how-do-you-incorporate-stemming-lemmatization?lang=fr&lang=fr)\n",
    "[39](https://spacy.io/usage/spacy-101)\n",
    "[40](https://www.kaggle.com/datasets/rtatman/stopword-lists-for-19-languages)\n",
    "[41](https://www.geeksforgeeks.org/nlp/lemmatization-vs-stemming-a-deep-dive-into-nlps-text-normalization-techniques/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6749d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
