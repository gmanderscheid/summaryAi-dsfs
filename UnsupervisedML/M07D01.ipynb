{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b14b42d",
   "metadata": {},
   "source": [
    "# Machine Learning Non Supervisé : Clustering et Réduction de Dimensionnalité\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/UnSupervisedML/M07D01_DataScience.m4a\n",
    "\n",
    "Ce cours de 3 heures explore les concepts fondamentaux de l'apprentissage non supervisé, avec un focus particulier sur l'algorithme K-means, les métriques d'évaluation des clusters, et l'analyse en composantes principales (PCA).\n",
    "\n",
    "## **Introduction à l'Apprentissage Non Supervisé**\n",
    "\n",
    "### **Définition Fondamentale**\n",
    "\n",
    "L'apprentissage non supervisé se distingue par **l'absence de variable cible**. Contrairement au machine learning supervisé qui prédit des étiquettes connues, le non supervisé cherche à **identifier les patterns sous-jacents dans les données**. L'objectif principal est de découvrir la structure cachée des données sans supervision humaine préalable.[1][2]\n",
    "\n",
    "**Caractéristiques essentielles** :[3][4]\n",
    "- Utilisation de données structurées mais non étiquetées\n",
    "- Découverte automatique de patterns et relations\n",
    "- Pas de fonction de coût basée sur des prédictions correctes\n",
    "- Focus sur l'analyse exploratoire des données\n",
    "\n",
    "### **Applications Concrètes en Entreprise**\n",
    "\n",
    "**Segmentation clientèle** : Une compagnie aérienne peut analyser les commentaires Twitter pour identifier automatiquement les sujets de mécontentement (retards, service, espace) sans connaître à l'avance ces catégories.[1]\n",
    "\n",
    "**Recommandation musicale** : Spotify utilise le clustering pour regrouper des utilisateurs aux goûts similaires. Si un groupe écoute beaucoup un nouveau titre, il sera recommandé aux autres membres du groupe.[1]\n",
    "\n",
    "**Catégorisation produits** : Walmart peut automatiquement regrouper ses millions d'articles par caractéristiques similaires (poids, dimensions, prix) plutôt que par catégories conceptuelles traditionnelles.[1]\n",
    "\n",
    "## **L'Algorithme K-means : Principe et Fonctionnement**\n",
    "\n",
    "### **Conceptualisation Visuelle**\n",
    "\n",
    "K-means identifie des **groupes denses et éloignés** les uns des autres. Visuellement, on reconnaît un cluster par deux critères :[5][1]\n",
    "1. **Densité interne** : points proches les uns des autres\n",
    "2. **Séparation externe** : espaces vides entre les groupes\n",
    "\n",
    "**Exemple concret** : Dans un graphique salaire/expérience, K-means détectera naturellement les groupes de juniors (faible salaire, peu d'expérience) et seniors (salaire élevé, expérience importante).[1]\n",
    "\n",
    "### **Algorithme Détaillé**\n",
    "\n",
    "**Étape 1 : Initialisation**[6][5]\n",
    "- Choisir K (nombre de clusters souhaité)\n",
    "- Placer aléatoirement K centroïdes dans l'espace des données\n",
    "- Les centroïdes sont contraints par les valeurs min/max des features\n",
    "\n",
    "**Étape 2 : Attribution**[5][1]\n",
    "- Calculer la distance entre chaque point et tous les centroïdes\n",
    "- Assigner chaque point au centroïde le plus proche\n",
    "- Si 100 000 points et 3 centroïdes = 300 000 calculs de distance\n",
    "\n",
    "**Étape 3 : Recalcul des centroïdes**[5][1]\n",
    "- Calculer la moyenne de tous les points de chaque cluster\n",
    "- Déplacer le centroïde vers cette moyenne\n",
    "\n",
    "**Étape 4 : Convergence**[5]\n",
    "- Répéter les étapes 2-3 jusqu'à ce que les centroïdes ne bougent plus\n",
    "- Critère d'arrêt : stabilisation des positions ou nombre max d'itérations atteint\n",
    "\n",
    "### **Cas Limites et Défis**\n",
    "\n",
    "**Formes non-sphériques** : K-means échoue sur des clusters en forme de croissant ou de donuts car il crée naturellement des partitions circulaires autour des centroïdes.[1]\n",
    "\n",
    "**Convergence difficile** : Avec certaines formes de données, l'algorithme peut osciller entre positions sans jamais converger, d'où l'importance du paramètre `max_iter`.[1]\n",
    "\n",
    "## **Métriques d'Évaluation des Clusters**\n",
    "\n",
    "### **Within Cluster Sum of Squares (WCSS)**\n",
    "\n",
    "**Définition mathématique** :[7][8]\n",
    "$$ \\text{WCSS} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\text{distance}(x_j^{(i)}, c_i)^2 $$\n",
    "\n",
    "La WCSS mesure la **densité intra-cluster** en calculant la somme des distances au carré entre chaque point et le centroïde de son cluster.[7][1]\n",
    "\n",
    "**Propriété critique** : La WCSS diminue **mécaniquement** avec l'augmentation du nombre de clusters. Avec K=nombre de points, WCSS=0 car chaque point est son propre centroïde.[9][1]\n",
    "\n",
    "**Exemple pratique** : Sur un dataset de 1000 points, passer de 2 à 3 clusters réduira toujours la WCSS, mais cette réduction peut devenir marginale.\n",
    "\n",
    "### **Silhouette Score**\n",
    "\n",
    "**Formulation mathématique** :[10][11]\n",
    "$$ \\text{Silhouette}(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "\n",
    "Où :\n",
    "- **a(i)** : distance intra-cluster moyenne (proximité au cluster d'appartenance)\n",
    "- **b(i)** : distance au cluster le plus proche externe\n",
    "\n",
    "**Interprétation des valeurs** :[12][10]\n",
    "- **+1** : Point parfaitement assigné à son cluster\n",
    "- **0** : Point à la frontière entre clusters\n",
    "- **-1** : Point mal assigné (devrait être dans un autre cluster)\n",
    "\n",
    "**Seuils pratiques** :[12]\n",
    "- **> 0.7** : clustering \"fort\"\n",
    "- **0.5-0.7** : clustering \"raisonnable\"\n",
    "- **0.25-0.5** : clustering \"faible\"\n",
    "\n",
    "### **Méthode du Coude (Elbow Method)**\n",
    "\n",
    "**Principe visuel** : Tracer la WCSS en fonction du nombre de clusters K. L'optimal se situe au \"coude\" où la courbe change brutalement de pente.[8][9]\n",
    "\n",
    "**Limitation principale** : La méthode du coude ne considère que la densité intra-cluster, ignorant la séparation inter-clusters.[9]\n",
    "\n",
    "**Approche combinée recommandée** :\n",
    "1. Identifier les candidats K avec la méthode du coude\n",
    "2. Valider avec le silhouette score pour la séparation\n",
    "3. Considérer les contraintes business (budget marketing, capacité d'analyse)\n",
    "\n",
    "## **Notions de Distance en Clustering**\n",
    "\n",
    "### **Distance Euclidienne (L2)**\n",
    "\n",
    "**Formule** :[13][14]\n",
    "$$ d_{euclidienne} = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} $$\n",
    "\n",
    "**Caractéristiques** :[15][14]\n",
    "- Distance \"à vol d'oiseau\", ligne droite\n",
    "- Sensible aux outliers (effet des carrés)\n",
    "- Standard par défaut dans K-means\n",
    "\n",
    "### **Distance de Manhattan (L1)**\n",
    "\n",
    "**Formule** :[14][13]\n",
    "$$ d_{manhattan} = \\sum_{i=1}^n |x_i-y_i| $$\n",
    "\n",
    "**Avantages spécifiques** :[15][14]\n",
    "- Plus robuste en haute dimension\n",
    "-适合 variables catégorielles discrètes  \n",
    "- Moins sensible aux outliers\n",
    "\n",
    "**Exemple visuel** : Dans une ville avec des rues en grille, la distance de Manhattan correspond au trajet réel en voiture.[15]\n",
    "\n",
    "### **Distance de Minkowski (généralisation)**\n",
    "\n",
    "**Formule** :[14][15]\n",
    "$$ d_{minkowski} = \\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p} $$\n",
    "\n",
    "- **p=1** : Distance de Manhattan\n",
    "- **p=2** : Distance euclidienne  \n",
    "- **p→∞** : Distance de Tchebychev (maximum des différences)\n",
    "\n",
    "### **Distance de Levenshtein (texte)**\n",
    "\n",
    "**Application NLP** : Mesure le nombre minimal d'opérations (insertion, suppression, substitution) pour transformer une chaîne en autre.[16]\n",
    "\n",
    "**Exemples** :\n",
    "- \"Guillaume\" → \"Guillaume\" : distance = 1\n",
    "- \"car\" → \"care\" : distance = 1 (mais sens complètement différent !)\n",
    "\n",
    "**Limitation critique** : Coût computationnel prohibitif (O(n×m) pour chaque paire de mots) et problèmes sémantiques (homonymes, synonymes).[1]\n",
    "\n",
    "## **Optimisation du Nombre de Clusters**\n",
    "\n",
    "### **Processus de Sélection Guidé par les Données**\n",
    "\n",
    "**Workflow recommandé** :[1]\n",
    "\n",
    "```python\n",
    "# Tester une range de K\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss_values.append(kmeans.inertia_)\n",
    "    silhouette_values.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "```\n",
    "\n",
    "### **Interprétation Pratique**\n",
    "\n",
    "**Cas d'étude Iris** :[1]\n",
    "- **K=2** : Silhouette score maximal mais WCSS élevée (clusters peu denses)\n",
    "- **K=3** : Équilibre optimal - silhouette correcte et WCSS acceptable\n",
    "- **K=4+** : Dégradation du silhouette sans gain significatif en densité\n",
    "\n",
    "**Contraintes Business** : Si l'objectif est de créer 15 campagnes marketing distinctes mais que l'analyse suggère 3 clusters, privilégier la contrainte métier avec des sous-segmentations.[1]\n",
    "\n",
    "## **Analyse en Composantes Principales (PCA)**\n",
    "\n",
    "### **Objectif et Principe**\n",
    "\n",
    "La PCA résout le problème de la **haute dimensionnalité** en créant de nouveaux axes qui **maximisent la variance expliquée**.[17][18][19]\n",
    "\n",
    "**Processus de transformation** :[18][20]\n",
    "1. Centrer les données (moyenne = 0)\n",
    "2. Calculer la matrice de covariance\n",
    "3. Extraire les vecteurs propres (directions de variance maximale)\n",
    "4. Projeter les données sur ces nouveaux axes\n",
    "\n",
    "### **Interprétation des Composantes**\n",
    "\n",
    "**Variance expliquée** :[1]\n",
    "- PC1 : 72% de la variance totale\n",
    "- PC2 : 23% de la variance  \n",
    "- PC3 : 4% de la variance\n",
    "- PC4 : 1% de la variance\n",
    "\n",
    "**Trade-off dimension/information** :[1]\n",
    "- **Visualisation** : Garder 2-3 composantes (99%+ variance pour voir en 2D/3D)\n",
    "- **Machine Learning** : Garder jusqu'à 95-96% de variance totale\n",
    "- **Compression** : Accepter plus de perte selon le cas d'usage\n",
    "\n",
    "### **Applications Pratiques**\n",
    "\n",
    "**Compression d'images** : Une image 150×150 pixels peut être représentée avec 15-20 composantes principales en gardant 95% de l'information visuelle.[1]\n",
    "\n",
    "**Préprocessing pour clustering** : Réduire 150 dimensions à 15 accélère K-means tout en préservant l'essentiel des patterns.[1]\n",
    "\n",
    "## **Algorithmes Complémentaires**\n",
    "\n",
    "### **DBSCAN (Density-Based Spatial Clustering)**\n",
    "\n",
    "**Avantages sur K-means** :[21][22]\n",
    "- Détecte automatiquement le nombre de clusters\n",
    "- Gère les formes arbitraires (non-sphériques)\n",
    "- Identifie et exclut le bruit (outliers)\n",
    "\n",
    "**Limitations spécifiques** :[1]\n",
    "- Pas de méthode `predict()` native\n",
    "- Nécessite de relancer l'algorithme complet pour de nouveaux points\n",
    "- Difficile à utiliser en production pour des prédictions\n",
    "\n",
    "**Paramètres critiques** :[23][21]\n",
    "- **eps** : rayon du voisinage\n",
    "- **min_samples** : nombre minimum de points pour former un cluster dense\n",
    "\n",
    "## **Active Learning et Applications Avancées**\n",
    "\n",
    "### **Principe de l'Active Learning**\n",
    "\n",
    "**Processus itératif** :[24][25]\n",
    "1. Entraîner un modèle sur peu de données étiquetées\n",
    "2. Identifier les points avec la **confiance la plus faible**\n",
    "3. Faire étiqueter ces points par des humains\n",
    "4. Réentraîner le modèle\n",
    "5. Répéter jusqu'à satisfaction\n",
    "\n",
    "### **Économies Réalisées**\n",
    "\n",
    "**Exemple concret** : Plutôt que d'étiqueter 1 million de points à 100€/mois par annotateur, l'active learning permet d'atteindre 95% de performance avec seulement 50 000 étiquetages ciblés.[1]\n",
    "\n",
    "**Applications industrielles** :[25][24]\n",
    "- Diagnostic médical (sélection des cas les plus informatifs)\n",
    "- Classification d'images (focus sur les cas ambigus)\n",
    "- NLP (identification des textes les plus représentatifs)\n",
    "\n",
    "## **Workflow Pratique et Code**\n",
    "\n",
    "### **Pipeline Standard K-means**\n",
    "\n",
    "```python\n",
    "# 1. Standardisation (obligatoire)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Boucle d'optimisation K\n",
    "wcss_values = []\n",
    "silhouette_values = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss_values.append(kmeans.inertia_)\n",
    "    silhouette_values.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# 3. Sélection du K optimal\n",
    "# Analyser graphiques + contraintes business\n",
    "\n",
    "# 4. Modèle final\n",
    "final_kmeans = KMeans(n_clusters=optimal_k)\n",
    "clusters = final_kmeans.fit_predict(X_scaled)\n",
    "```\n",
    "\n",
    "### **Intégration avec PCA pour Visualisation**\n",
    "\n",
    "```python\n",
    "# Réduction dimensionnelle\n",
    "pca = PCA(n_components=3)  # Pour vis 3D\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Clustering sur données PCA\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Récupération des centroïdes en coordonnées originales\n",
    "centroids_original = scaler.inverse_transform(\n",
    "    pca.inverse_transform(kmeans.cluster_centers_)\n",
    ")\n",
    "```\n",
    "\n",
    "## **Considérations Métier et Déploiement**\n",
    "\n",
    "### **Machine Learning Non Supervisé comme Étape Intermédiaire**\n",
    "\n",
    "Le clustering est **rarement une fin en soi** mais plutôt une étape dans un processus plus large :[1]\n",
    "- Création de features pour modèles supervisés\n",
    "- Pré-segmentation pour analyses approfondies  \n",
    "- Base pour stratégies marketing ciblées\n",
    "\n",
    "### **Évolution vs Machine Learning Supervisé**\n",
    "\n",
    "**Différences fondamentales** :[1]\n",
    "- **Supervisé** : Score objectif à optimiser, résultat directement actionable\n",
    "- **Non supervisé** : Interprétation subjective requise, étape exploratoire\n",
    "\n",
    "**Implications pratiques** : Le succès du clustering dépend autant de l'expertise métier que de la performance algorithmique pour interpréter et actionner les résultats.\n",
    "\n",
    "Cette approche méthodologique du machine learning non supervisé permet une compréhension robuste des patterns cachés dans les données, tout en gardant un ancrage pratique avec les contraintes et objectifs business.\n",
    "\n",
    "[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/76884410/7de3b099-9f91-47ba-80fa-8cdea9851028/transcriptM07D01.txt)\n",
    "[2](https://fr.mathworks.com/discovery/unsupervised-learning.html)\n",
    "[3](https://cloud.google.com/discover/what-is-unsupervised-learning?hl=fr)\n",
    "[4](https://www.ibm.com/fr-fr/think/topics/unsupervised-learning)\n",
    "[5](https://mrmint.fr/algorithme-k-means)\n",
    "[6](https://www.ibm.com/fr-fr/think/topics/k-means-clustering)\n",
    "[7](https://opendatascience.com/unsupervised-learning-evaluating-clusters/)\n",
    "[8](https://www.geeksforgeeks.org/machine-learning/elbow-method-for-optimal-value-of-k-in-kmeans/)\n",
    "[9](https://builtin.com/data-science/elbow-method)\n",
    "[10](https://www.geeksforgeeks.org/machine-learning/what-is-silhouette-score/)\n",
    "[11](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)\n",
    "[12](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "[13](https://fr.wikipedia.org/wiki/Distance_(math%C3%A9matiques))\n",
    "[14](https://penseeartificielle.fr/choisir-distance-machine-learning/)\n",
    "[15](https://fr.linkedin.com/advice/3/what-most-effective-distance-metrics-optimizing-xndwc?lang=fr&lang=fr)\n",
    "[16](https://fr.wikipedia.org/wiki/Distance_de_Levenshtein)\n",
    "[17](https://www.datarockstars.ai/principal-component-analysis-pca/)\n",
    "[18](https://dridk.me/analyse-en-composante-principale.html)\n",
    "[19](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)\n",
    "[20](https://spss.espaceweb.usherbrooke.ca/analyse-en-composantes-principales-2/)\n",
    "[21](https://www.geeksforgeeks.org/machine-learning/dbscan-clustering-in-ml-density-based-clustering/)\n",
    "[22](https://www.ultralytics.com/glossary/dbscan-density-based-spatial-clustering-of-applications-with-noise)\n",
    "[23](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "[24](https://www.geeksforgeeks.org/machine-learning/ml-active-learning/)\n",
    "[25](https://encord.com/blog/active-learning-machine-learning-guide/)\n",
    "[26](https://www.oracle.com/fr/artificial-intelligence/machine-learning/unsupervised-learning/)\n",
    "[27](https://www.jedha.co/formation-ia/algorithme-kmeans)\n",
    "[28](https://www.jedha.co/blog/les-differents-types-de-machine-learning)\n",
    "[29](https://datascientest.com/algorithme-des-k-means)\n",
    "[30](https://aws.amazon.com/fr/compare/the-difference-between-machine-learning-supervised-and-unsupervised/)\n",
    "[31](https://blent.ai/blog/a/k-means-comment-ca-marche)\n",
    "[32](https://www.youtube.com/watch?v=y5hzRYZxd4Y)\n",
    "[33](https://fr.wikipedia.org/wiki/K-moyennes)\n",
    "[34](https://www.ibm.com/fr-fr/think/topics/principal-component-analysis)\n",
    "[35](https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9)\n",
    "[36](https://www.lesphinx-developpement.fr/blog/les-typologies-des-repondants-la-classification-k-means/)\n",
    "[37](https://www.linkedin.com/posts/vishal-pandey-186a23225_difference-between-wcsswithin-cluster-sum-activity-7056578152592007168-VDVt)\n",
    "[38](https://en.wikipedia.org/wiki/Elbow_method_(clustering))\n",
    "[39](https://www.youtube.com/watch?v=ht7geyMAFfA)\n",
    "[40](https://365datascience.com/tutorials/python-tutorials/k-means-clustering/)\n",
    "[41](https://fr.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "[42](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)\n",
    "[43](https://systemds.apache.org/docs/2.1.0/site/algorithms-clustering.html)\n",
    "[44](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "[45](https://fr.linkedin.com/advice/0/how-do-you-choose-best-k-elbow-method-cluster?lang=fr&lang=fr)\n",
    "[46](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "[47](https://fr.linkedin.com/advice/0/how-can-you-calculate-silhouette-score-clustering-algorithm-w9bcc?lang=fr&lang=fr)\n",
    "[48](https://www.kaggle.com/code/jasleensondhi/k-means-clustering-on-iris-data-elbow-method)\n",
    "[49](https://www.sciencedirect.com/science/article/pii/S1877050920318469)\n",
    "[50](https://spssanalysis.com/silhouette-cluster-analysis-in-spss/)\n",
    "[51](https://en.wikipedia.org/wiki/Active_learning_(machine_learning))\n",
    "[52](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "[53](https://www.datarobot.com/blog/active-learning-machine-learning/)\n",
    "[54](https://datascientest.com/machine-learning-clustering-dbscan)\n",
    "[55](http://www.jybaudot.fr/Analdonnees/metriques.html)\n",
    "[56](https://arxiv.org/pdf/2009.00236.pdf)\n",
    "[57](https://fr.wikipedia.org/wiki/DBSCAN)\n",
    "[58](https://www.youtube.com/watch?v=siBRplE4bs4)\n",
    "[59](https://www.lxt.ai/ai-glossary/active-learning/)\n",
    "[60](https://cedric.cnam.fr/vertigo/Cours/ml/docs/3coursDBSCAN.pdf)\n",
    "[61](https://www.developpez.net/forums/showthread.php?t=940211)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4c4ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
