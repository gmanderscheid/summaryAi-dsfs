{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d6fb86",
   "metadata": {},
   "source": [
    "## Machine Learning : Optimisation de Code et Clustering Avancé\n",
    "\n",
    "Ce cours de 3 heures explore les aspects avancés de l'optimisation de code Python et l'algorithme de clustering DBSCAN, offrant une approche approfondie des techniques d'amélioration des performances et des méthodes de clustering basées sur la densité.\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/UnSupervisedML/M07D02_DataScience.m4a\n",
    "\n",
    "## **Introduction et Contexte Pédagogique**\n",
    "\n",
    "L'approche du cours suit une philosophie claire : **\"S'il y a une boucle forte, on essaie de l'enlever\"**. Cette maxime reflète l'importance de l'optimisation en machine learning, particulièrement cruciale lorsque l'on travaille avec de grandes quantités de données.[1]\n",
    "\n",
    "Le cours combine théorie et pratique en se basant sur des **exemples concrets d'optimisation** observés en temps réel, démontrant comment transformer du code fonctionnel mais lent en solutions performantes.\n",
    "\n",
    "## **Optimisation Fondamentale du Code Python**\n",
    "\n",
    "### **Problématiques des Boucles Traditionnelles**\n",
    "\n",
    "Les boucles Python présentent des limitations intrinsèques :[2][3]\n",
    "\n",
    "**Surcoût d'interprétation** : Python interprète chaque itération individuellement, créant une overhead computationnelle significative. Pour 1000 points et 3 centroïdes, cela représente 3000 calculs de distance par itération.[1]\n",
    "\n",
    "**Masquage répétitif** : L'application de filtres (masques) de manière répétée sur les mêmes données devient particulièrement coûteuse. Chaque `mask = (cluster_labels == label)` doit parcourir l'intégralité du dataset.[1]\n",
    "\n",
    "### **Solution : Module Collections**\n",
    "\n",
    "Le module `collections.Counter` révolutionne le comptage d'occurrences. Contrairement aux dictionnaires traditionnels, Counter offre des méthodes spécialisées :[4][5]\n",
    "\n",
    "**most_common(n)** : Extrait les n éléments les plus fréquents en temps linéaire. Cette méthode évite le tri complet et l'indexation manuelle.[6]\n",
    "\n",
    "**Opérations arithmétiques** : Permet l'addition et la soustraction directe entre Counter, facilitant les analyses comparatives.[4]\n",
    "\n",
    "### **Transformation Pratique d'Optimisation**\n",
    "\n",
    "**Version initiale (lente)** :\n",
    "```python\n",
    "# Approche avec boucles explicites - O(k*n)\n",
    "for cluster_id in unique_clusters:\n",
    "    mask = (cluster_labels == cluster_id)  # Coûteux\n",
    "    target_counts = data[mask]['target'].value_counts()\n",
    "    most_frequent_target = target_counts.index[0]\n",
    "    cluster_mapping[cluster_id] = most_frequent_target\n",
    "```\n",
    "\n",
    "**Version optimisée** :\n",
    "```python\n",
    "# Groupby + Counter - O(n)\n",
    "grouped_data = df.groupby('cluster_label')['target'].apply(Counter)\n",
    "optimized_mapping = {\n",
    "    cluster: counter.most_common(1)[0][0] \n",
    "    for cluster, counter in grouped_data.items()\n",
    "}\n",
    "```\n",
    "\n",
    "Les **tests de performance** réalisés en cours montrent une **amélioration de 5x** sur des datasets volumineux, avec une complexité réduite de O(k*n) à O(n).[1]\n",
    "\n",
    "## **Révision Approfondie de K-means**\n",
    "\n",
    "### **Mécanisme Algorithmique Détaillé**\n",
    "\n",
    "L'algorithme K-means suit un processus itératif rigoureux :[1]\n",
    "\n",
    "1. **Initialisation aléatoire** des K centroïdes dans l'espace des features\n",
    "2. **Attribution** : Calcul de 3000 distances pour 1000 points et 3 centroïdes\n",
    "3. **Recalcul** : Centroïdes repositionnés à la moyenne de leurs clusters\n",
    "4. **Convergence** : Répétition jusqu'à stabilisation ou limite d'itérations\n",
    "\n",
    "### **WCSS - Compréhension Mathématique**\n",
    "\n",
    "La **Within Cluster Sum of Squares** suit une propriété mathématique fondamentale :[7][8]\n",
    "\n",
    "$$ \\text{WCSS} = \\sum_{i=1}^{k} \\sum_{j \\in C_i} ||x_j - \\mu_i||^2 $$\n",
    "\n",
    "**Propriété critique** : WCSS décroît mécaniquement avec k. Avec k=nombre de points, WCSS=0 car chaque point devient son propre centroïde. Cette propriété explique pourquoi la méthode du coude recherche un **changement de pente significatif** plutôt qu'une valeur minimale absolue.[1]\n",
    "\n",
    "### **Silhouette Score - Interprétation Avancée**\n",
    "\n",
    "Le calcul du Silhouette Score pour chaque point i :[9][10]\n",
    "\n",
    "$$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n",
    "\n",
    "**Points en bordure** : Lorsqu'un point se situe à la frontière entre clusters, son score peut devenir négatif. Cela ne signifie pas nécessairement une mauvaise assignation, mais plutôt une **proximité inter-clusters**.[1]\n",
    "\n",
    "**Silhouette du modèle** : La moyenne arithmétique des scores individuels, pénalisée par les points en bordure ayant des valeurs négatives.\n",
    "\n",
    "## **DBSCAN - Clustering par Densité**\n",
    "\n",
    "### **Limitations Structurelles de K-means**\n",
    "\n",
    "K-means crée invariablement des **partitions sphériques** autour des centroïdes. Cette limitation devient évidente avec des données en forme de :[11]\n",
    "- **Croissants de lune** (make_moons dataset)[12][13]\n",
    "- **Clusters allongés** ou de formes arbitraires\n",
    "- **Densités variables** dans l'espace des features\n",
    "\n",
    "### **Paradigme de Densité**\n",
    "\n",
    "DBSCAN révolutionne l'approche en se basant sur la **densité locale** plutôt que sur la proximité à un centre. Cette approche permet de :[14][11]\n",
    "- Détecter des clusters de **formes arbitraires**\n",
    "- Identifier automatiquement les **outliers**\n",
    "- Adapter le clustering à la **structure intrinsèque** des données\n",
    "\n",
    "### **Taxonomie des Points**\n",
    "\n",
    "**Core Points** : Points possédant ≥ min_samples voisins dans un rayon ε. Ces points forment la **colonne vertébrale** des clusters.[15][11]\n",
    "\n",
    "**Border Points** : Points avec < min_samples voisins mais situés dans le rayon ε d'un core point. Ils **appartiennent au cluster** mais ne peuvent pas l'étendre.[11]\n",
    "\n",
    "**Noise Points** : Points isolés n'appartenant à aucun cluster, étiquetés -1. Représentent les **outliers naturels** du dataset.[11]\n",
    "\n",
    "### **Processus Algorithmique**\n",
    "\n",
    "L'algorithme suit une **expansion de proche en proche** :[1]\n",
    "\n",
    "1. **Sélection** d'un point non-visité\n",
    "2. **Comptage** des voisins dans le rayon ε\n",
    "3. **Classification** : Core, Border, ou potentiel Noise\n",
    "4. **Expansion** : Propagation du cluster vers tous les points density-reachable\n",
    "5. **Itération** jusqu'à épuisement des points\n",
    "\n",
    "**Exemple concret** avec ε=0.2, min_samples=4 :[1]\n",
    "- Point A : 5 voisins → Core point → Initie Cluster 1\n",
    "- Point B : 3 voisins mais dans rayon de A → Border point → Rejoint Cluster 1\n",
    "- Point C : 2 voisins, isolé → Noise point → Étiquette -1\n",
    "\n",
    "### **Optimisation des Paramètres**\n",
    "\n",
    "#### **Méthode k-distance**\n",
    "\n",
    "La détermination optimale d'epsilon utilise l'analyse des **k-plus-proches-voisins** :[16][17]\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors.kneighbors(X_scaled)\n",
    "\n",
    "# k-distances triées\n",
    "k_distances = np.sort(distances[:, min_samples-1])\n",
    "```\n",
    "\n",
    "**Interprétation graphique** :[17]\n",
    "- **Plateau initial** : Points dans des clusters denses\n",
    "- **Coude prononcé** : Transition vers les outliers  \n",
    "- **Montée abrupte** : Points complètement isolés\n",
    "\n",
    "L'epsilon optimal se situe généralement **au niveau du coude**, représentant la distance caractéristique intra-cluster.\n",
    "\n",
    "#### **Impact Paramétrique**\n",
    "\n",
    "**ε trop petit** :[18][1]\n",
    "- Fragmentation excessive des clusters\n",
    "- Classification de nombreux points comme noise\n",
    "- Perte d'informations structurelles\n",
    "\n",
    "**ε trop grand** :[1]\n",
    "- Fusion de clusters distincts\n",
    "- Réduction artificielle du nombre de groupes\n",
    "- Possible création d'un unique mega-cluster\n",
    "\n",
    "**min_samples critique** :[1]\n",
    "- **Trop petit** (1-2) : Sensibilité excessive au bruit\n",
    "- **Trop grand** (>10% du dataset) : Seules les zones très denses forment des clusters\n",
    "\n",
    "## **Exemples Pratiques et Implémentation**\n",
    "\n",
    "### **Dataset make_moons**\n",
    "\n",
    "Le dataset make_moons génère **deux croissants entrelacés**, parfait pour illustrer les limitations de K-means :[13][12]\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "```\n",
    "\n",
    "**Paramètres cruciaux** :\n",
    "- **noise** : Contrôle la dispersion gaussienne (0.05 = peu de bruit, 0.5 = très dispersé)\n",
    "- **n_samples** : Nombre total de points (distribution équitable sur les deux croissants)\n",
    "\n",
    "### **Comparaison Algorithmique**\n",
    "\n",
    "**K-means sur make_moons** : Crée invariablement une séparation linéaire inadéquate, ignorant la courbure naturelle des croissants.[1]\n",
    "\n",
    "**DBSCAN sur make_moons** : S'adapte parfaitement à la forme courbe, respectant la structure géométrique intrinsèque des données.\n",
    "\n",
    "### **Optimisation Avancée avec NearestNeighbors**\n",
    "\n",
    "```python\n",
    "def analyze_epsilon_range(X, min_samples_target):\n",
    "    \"\"\"\n",
    "    Analyse les distances pour suggérer une plage d'epsilon pertinente\n",
    "    \"\"\"\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples_target)\n",
    "    neighbors.fit(X)\n",
    "    distances, _ = neighbors.kneighbors(X)\n",
    "    \n",
    "    k_distances = distances[:, min_samples_target-1]\n",
    "    \n",
    "    # Statistiques descriptives pour guider le choix\n",
    "    percentiles = np.percentile(k_distances, [25, 50, 75, 90])\n",
    "    \n",
    "    return {\n",
    "        'min_distance': k_distances.min(),\n",
    "        'median_distance': percentiles[1], \n",
    "        'suggested_range': (percentiles[0], percentiles[2]),\n",
    "        'noise_threshold': percentiles[3]\n",
    "    }\n",
    "```\n",
    "\n",
    "Cette analyse **évite les tests aléatoires** d'epsilon en fournissant une plage statistiquement fondée.[1]\n",
    "\n",
    "### **Évaluation Comparative**\n",
    "\n",
    "```python\n",
    "def comprehensive_clustering_comparison(X, true_labels=None):\n",
    "    \"\"\"\n",
    "    Comparaison exhaustive K-means vs DBSCAN\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # K-means configuration\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # DBSCAN avec paramètres optimisés\n",
    "    epsilon_analysis = analyze_epsilon_range(X_scaled, 5)\n",
    "    optimal_eps = epsilon_analysis['median_distance']\n",
    "    \n",
    "    dbscan = DBSCAN(eps=optimal_eps, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Métriques comparatives\n",
    "    metrics = {}\n",
    "    \n",
    "    if true_labels is not None:\n",
    "        from sklearn.metrics import adjusted_rand_score, homogeneity_score\n",
    "        metrics['K-means_ARI'] = adjusted_rand_score(true_labels, kmeans_labels)\n",
    "        metrics['DBSCAN_ARI'] = adjusted_rand_score(true_labels, dbscan_labels)\n",
    "    \n",
    "    # Silhouette scores\n",
    "    metrics['K-means_Silhouette'] = silhouette_score(X_scaled, kmeans_labels)\n",
    "    \n",
    "    # DBSCAN silhouette uniquement si clusters valides\n",
    "    if len(set(dbscan_labels)) > 1 and -1 not in set(dbscan_labels):\n",
    "        metrics['DBSCAN_Silhouette'] = silhouette_score(X_scaled, dbscan_labels)\n",
    "    \n",
    "    # Informations structurelles\n",
    "    metrics['DBSCAN_Noise_Count'] = list(dbscan_labels).count(-1)\n",
    "    metrics['DBSCAN_Noise_Percentage'] = (list(dbscan_labels).count(-1) / len(dbscan_labels)) * 100\n",
    "    \n",
    "    return metrics, {'kmeans': kmeans_labels, 'dbscan': dbscan_labels}\n",
    "```\n",
    "\n",
    "## **Considérations Avancées et Recommandations**\n",
    "\n",
    "### **Choix Algorithmique Contextuel**\n",
    "\n",
    "**K-means - Privilégier quand** :\n",
    "- Clusters sphériques et de tailles similaires\n",
    "- Nombre de clusters connu a priori  \n",
    "- Vitesse d'exécution critique\n",
    "- Données sans outliers significatifs\n",
    "\n",
    "**DBSCAN - Recommandé pour** :\n",
    "- Formes de clusters arbitraires et complexes\n",
    "- Détection d'outliers requise\n",
    "- Densité variable dans l'espace des features\n",
    "- Nombre de clusters inconnu ou variable\n",
    "\n",
    "### **Standardisation Obligatoire**\n",
    "\n",
    "**Pour DBSCAN** : La standardisation est **absolument critique**. Les différences d'échelles entre features créent des distorsions dans le calcul d'epsilon, pouvant complètement fausser la détection de densité.[1]\n",
    "\n",
    "**Pour K-means** : Recommandée pour éviter la domination par les features à plus grande amplitude.\n",
    "\n",
    "### **Limitations et Extensions**\n",
    "\n",
    "**DBSCAN sans predict()** : Contrairement à K-means, DBSCAN ne possède pas de méthode `predict()` native. Pour classifier de nouveaux points, il faut **relancer l'algorithme complet** ou implémenter une logique de proximité aux core points existants.[1]\n",
    "\n",
    "**Solutions alternatives** :\n",
    "- **HDBSCAN** : Version hiérarchique gérant les densités variables\n",
    "- **OPTICS** : Ordering Points To Identify Clustering Structure\n",
    "- **Mean Shift** : Clustering basé sur la recherche de modes\n",
    "\n",
    "## **Impact Pédagogique et Méthodologique**\n",
    "\n",
    "Ce cours illustre parfaitement la progression naturelle d'apprentissage en machine learning : partir d'algorithmes conceptuellement simples (K-means) pour comprendre les limitations fondamentales, puis explorer des solutions plus sophistiquées (DBSCAN) répondant à ces limitations spécifiques.\n",
    "\n",
    "L'approche d'optimisation de code démontre également une compétence cruciale en data science : **l'optimisation itérative**. Commencer par une solution fonctionnelle, identifier les goulots d'étranglement, puis appliquer des techniques d'optimisation ciblées.\n",
    "\n",
    "### **Ressources Complémentaires Recommandées**\n",
    "\n",
    "Le cours mentionne plusieurs ressources vidéo essentielles :[1]\n",
    "- **3Blue1Brown** : Visualisations mathématiques des vecteurs propres (préparation PCA)\n",
    "- **StatQuest** : Explications visuelles des concepts de clustering  \n",
    "- **Free Calculus** : Fondements mathématiques\n",
    "\n",
    "Ces ressources préparent efficacement aux concepts avancés de réduction de dimensionnalité qui suivront dans le curriculum.\n",
    "\n",
    "[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/76884410/49c1568b-4c18-4f98-82fb-95454117be10/transcriptM07D02.txt)\n",
    "[2](https://perso.univ-lyon1.fr/marc.buffat/COURS/BOOK_OUTILSNUM_HTML/MGC2367M/cours/Chap2_Optimisation.html)\n",
    "[3](https://www.reddit.com/r/learnpython/comments/11d6o3e/how_to_make_nested_for_loops_run_faster/)\n",
    "[4](https://www.geeksforgeeks.org/python/counters-in-python-set-1/)\n",
    "[5](https://www.9raytifclick.com/cours/les-collections-en-python/)\n",
    "[6](https://koor.fr/Python/API/python/collections/Counter/Index.wp)\n",
    "[7](https://opendatascience.com/unsupervised-learning-evaluating-clusters/)\n",
    "[8](https://www.geeksforgeeks.org/machine-learning/elbow-method-for-optimal-value-of-k-in-kmeans/)\n",
    "[9](https://www.geeksforgeeks.org/machine-learning/what-is-silhouette-score/)\n",
    "[10](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "[11](https://www.geeksforgeeks.org/machine-learning/dbscan-clustering-in-ml-density-based-clustering/)\n",
    "[12](https://laxmikants.github.io/blog/neural-network-using-make-moons-dataset/)\n",
    "[13](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)\n",
    "[14](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "[15](https://3d.bk.tudelft.nl/courses/geo5017/slides/03_slides_Clustering.pdf)\n",
    "[16](https://www.sefidian.com/2022/12/18/how-to-determine-epsilon-and-minpts-parameters-of-dbscan-clustering/)\n",
    "[17](https://blog.dailydoseofds.com/p/how-to-find-optimal-epsilon-value)\n",
    "[18](https://stackoverflow.com/questions/26666367/scikit-dbscan-eps-and-min-sample-value-determination)\n",
    "[19](https://fr.linkedin.com/advice/1/how-can-you-optimize-performance-using-groupby-function-oxwbf?lang=fr)\n",
    "[20](https://www.reddit.com/r/learnpython/comments/zoz4z1/speeding_up_pandas_groupbyapply_or_looping/)\n",
    "[21](https://moncoachdata.com/blog/10-astuces-pour-accelerer-mes-scripts-python/)\n",
    "[22](https://stackoverflow.com/questions/47392758/improving-the-performance-of-pandas-groupby)\n",
    "[23](https://fr.linkedin.com/advice/1/how-can-you-optimize-game-loops-python-better-ycebc?lang=fr&lang=fr)\n",
    "[24](https://docs.python.org/fr/3.8/library/collections.html)\n",
    "[25](https://moncoachdata.com/blog/efficacite-et-performance-de-pandas/)\n",
    "[26](https://www.docstring.fr/blog/4-facons-doptimiser-votre-code-python/)\n",
    "[27](https://www.ionos.fr/digitalguide/sites-internet/developpement-web/python-counter/)\n",
    "[28](https://fr.linkedin.com/advice/0/how-do-you-troubleshoot-slow-groupby-operations-pandas-d7jtc?lang=fr)\n",
    "[29](https://blog.stephane-robert.info/docs/developper/programmation/python/linting/)\n",
    "[30](https://www.docstring.fr/blog/le-module-collections/)\n",
    "[31](https://www.youtube.com/watch?v=VPbqZNM7wTI)\n",
    "[32](https://www.data-bird.co/blog/boucle-for-python)\n",
    "[33](https://tainix.fr/code/Gestion-d-occurrences-avec-Counter-en-Python)\n",
    "[34](https://www.activeloop.ai/resources/glossary/radius-nearest-neighbors/)\n",
    "[35](https://scikit-learn-extra.readthedocs.io/en/stable/modules/cluster.html)\n",
    "[36](https://scikit-learn.org/0.16/modules/generated/sklearn.datasets.make_moons.html)\n",
    "[37](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "[38](https://python-course.eu/machine-learning/artificial-datasets-with-scikit-learn.php)\n",
    "[39](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "[40](https://www.kaggle.com/code/macespinoza/make-moons-python)\n",
    "[41](https://arxiv.org/abs/2212.07679)\n",
    "[42](https://datascientest.com/machine-learning-clustering-dbscan)\n",
    "[43](https://gist.github.com/Mehdi-Amine/2a57f9317068ba754dd1bba7b44ba94f)\n",
    "[44](https://en.wikipedia.org/wiki/Nearest_neighbor_search)\n",
    "[45](https://www.kaggle.com/code/ghazouanihaythem/dbscan-tutorial)\n",
    "[46](https://www.kaggle.com/code/nikhil25803/neural-network-make-moon-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc26b5d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
