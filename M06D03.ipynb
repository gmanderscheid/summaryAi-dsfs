{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7292b3be",
   "metadata": {},
   "source": [
    "# Régression Linéaire, Régularisation Lasso et Ridge\n",
    "---\n",
    "\n",
    "Lien vers l'audio : https://audio-records-dsfs.s3.eu-west-3.amazonaws.com/M06D03_DataScience.m4a\n",
    "\n",
    "## Introduction à la régression linéaire\n",
    "---\n",
    "\n",
    "La régression linéaire constitue le fondement de nombreux algorithmes d'apprentissage supervisé. Elle vise à modéliser la relation entre une variable dépendante et une ou plusieurs variables indépendantes par une fonction linéaire.\n",
    "\n",
    "### Principe des moindres carrés\n",
    "\n",
    "La méthode des moindres carrés minimise la somme des carrés des écarts entre les valeurs observées et les valeurs prédites par le modèle :\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "où $y_i$ représente la valeur réelle et $\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip}$ la valeur prédite.\n",
    "\n",
    "**Exemple concret :** Pour prédire le salaire en fonction de l'expérience, chaque point représente un individu. La droite de régression minimise la distance quadratique à tous les points, évitant que des écarts négatifs n'annulent des écarts positifs.\n",
    "\n",
    "**Analogie :** C'est comme tendre une corde au plus proche de plusieurs clous plantés à différentes hauteurs sur une planche : la meilleure position minimise la distance à chaque clou.\n",
    "\n",
    "## Évaluation des performances : R² et métriques\n",
    "---\n",
    "\n",
    "### Coefficient de détermination R²\n",
    "\n",
    "Le R² mesure la proportion de variance expliquée par le modèle :\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i}(y_i - \\hat{y}_i)^2}{\\sum_{i}(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- **R² = 0** : le modèle n'explique pas mieux que la moyenne\n",
    "- **R² = 1** : prédiction parfaite (souvent signe de surapprentissage)\n",
    "\n",
    "**Attention :** Un R² élevé n'est pas toujours synonyme de modèle utile, particulièrement en finance ou sur des séries temporelles avec des structures autocorrélées.\n",
    "\n",
    "### La notion de baseline (référence)\n",
    "\n",
    "La baseline constitue un modèle de référence simple (prédire la moyenne, la valeur précédente) permettant d'évaluer l'apport réel d'un modèle plus complexe.\n",
    "\n",
    "**Exemple :** Pour le Bitcoin, la baseline peut être \"le prix d'hier pour prédire aujourd'hui\". Un modèle n'apporte de valeur que s'il surpasse cette référence.\n",
    "\n",
    "**Analogie :** Lors du lancement d'un produit, la baseline représente \"les ventes sans campagne\" ; toute démarche supplémentaire doit prouver sa valeur ajoutée.\n",
    "\n",
    "## Problématique du surapprentissage\n",
    "---\n",
    "\n",
    "### Overfitting et Underfitting\n",
    "\n",
    "**L'overfitting** survient quand le modèle mémorise trop précisément les spécificités des données d'entraînement, échouant sur de nouveaux cas. C'est \"apprendre par cœur\" sans comprendre les patterns généraux.\n",
    "\n",
    "**L'underfitting** résulte d'un modèle trop simple qui ne capture pas la complexité sous-jacente des données.\n",
    "\n",
    "**Analogie :** L'overfitting ressemble à apprendre tous les itinéraires d'un quartier pour le permis, mais être incapable de conduire ailleurs. On n'a pas généralisé la compétence de conduite.\n",
    "\n",
    "### Stratégies de prévention\n",
    "\n",
    "- Réduire le nombre de variables (sélection de features)\n",
    "- Simplifier l'architecture du modèle\n",
    "- Traiter les valeurs aberrantes (outliers)\n",
    "- Augmenter le volume de données d'entraînement\n",
    "- **Régulariser le modèle** (approche détaillée ci-dessous)\n",
    "\n",
    "## Régularisation Ridge (L2)\n",
    "---\n",
    "\n",
    "### Principe et formulation mathématique\n",
    "\n",
    "La régularisation Ridge ajoute une pénalité proportionnelle au carré des coefficients à la fonction de coût :\n",
    "\n",
    "$$\n",
    "\\text{Coût}_{Ridge} = MSE + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "où $\\lambda$ (ou alpha) contrôle l'intensité de la régularisation.\n",
    "\n",
    "### Effet de la régularisation\n",
    "\n",
    "Ridge **réduit la magnitude** des coefficients sans les annuler complètement, stabilisant ainsi les prédictions face aux variations des données d'entraînement. Cette approche diminue la **variance** du modèle.\n",
    "\n",
    "**Exemple concret :** Pour prédire le prix d'une maison, Ridge évite qu'une seule variable (comme la surface) domine excessivement le modèle.\n",
    "\n",
    "**Analogie :** Comme fixer une limitation de vitesse sur autoroute : on peut rouler vite (coefficients importants), mais trop vite devient dangereux (surapprentissage).\n",
    "\n",
    "## Régularisation Lasso (L1)\n",
    "---\n",
    "\n",
    "### Formulation mathématique\n",
    "\n",
    "Lasso utilise la norme L1 comme terme de pénalité :\n",
    "\n",
    "$$\n",
    "\\text{Coût}_{Lasso} = MSE + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "### Propriété de sélection de variables\n",
    "\n",
    "Contrairement à Ridge, Lasso peut **forcer certains coefficients à zéro exact**, réalisant ainsi une sélection automatique de variables. Cette propriété résulte de la géométrie de la contrainte L1 (forme de losange vs cercle pour L2).\n",
    "\n",
    "**Exemple :** Sur un dataset de 300 variables, Lasso peut automatiquement sélectionner les 10 plus pertinentes en annulant les coefficients des autres.\n",
    "\n",
    "**Analogie :** Comme un chef qui goûte chaque épice d'un plat, puis décide d'en retirer complètement certaines pour ne garder que celles qui font réellement la différence gustative.\n",
    "\n",
    "## Optimisation des hyperparamètres\n",
    "---\n",
    "\n",
    "### Choix du paramètre λ (alpha)\n",
    "\n",
    "L'hyperparamètre λ contrôle l'équilibre biais-variance :\n",
    "- **λ faible** : peu de régularisation, risque d'overfitting\n",
    "- **λ élevé** : forte régularisation, risque d'underfitting\n",
    "\n",
    "### Grid Search et validation croisée\n",
    "\n",
    "La **validation croisée k-fold** évalue la robustesse du modèle en testant sur plusieurs partitions des données. Le **Grid Search** explore systématiquement une grille d'hyperparamètres.\n",
    "\n",
    "**Exemple de grille :**\n",
    "```python\n",
    "param_grid = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "```\n",
    "\n",
    "**Analogie :** Comme goûter plusieurs parts d'un gâteau réparties aléatoirement pour estimer sa qualité globale, plutôt que de juger sur une seule bouchée.\n",
    "\n",
    "### Interprétation des résultats\n",
    "\n",
    "Privilégier les hyperparamètres offrant :\n",
    "- **Score moyen élevé** sur la validation croisée\n",
    "- **Écart-type faible** (stabilité des performances)\n",
    "\n",
    "**Exemple :** Préférer un modèle à score moyen 0.80 ± 0.01 plutôt que 0.82 ± 0.09.\n",
    "\n",
    "## Métriques d'évaluation adaptées\n",
    "---\n",
    "\n",
    "### Au-delà du R²\n",
    "\n",
    "**Mean Absolute Error (MAE) :**\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Le MAE exprime l'erreur moyenne en unités compréhensibles (ex: \"5000€ d'écart moyen pour la prédiction de prix immobilier\").\n",
    "\n",
    "### Adaptation au contexte métier\n",
    "\n",
    "Le choix des métriques doit refléter les enjeux business :\n",
    "- **Finance** : privilégier les métriques directionnelles (signe de l'erreur)\n",
    "- **Immobilier** : MAE pour quantifier l'erreur monétaire\n",
    "- **Médecine** : métriques asymétriques selon le coût des erreurs\n",
    "\n",
    "## Synthèse et bonnes pratiques\n",
    "---\n",
    "\n",
    "### Workflow recommandé\n",
    "\n",
    "1. **Établir une baseline** simple avant d'explorer des modèles complexes\n",
    "2. **Évaluer biais vs variance** pour choisir entre Ridge (variance élevée) et Lasso (trop de variables)\n",
    "3. **Optimiser les hyperparamètres** par validation croisée\n",
    "4. **Sélectionner les métriques** adaptées au contexte métier\n",
    "5. **Documenter et itérer** le processus d'expérimentation\n",
    "\n",
    "### Principes directeurs\n",
    "\n",
    "- Toujours comparer au modèle de référence (baseline)\n",
    "- Privilégier la robustesse (faible variance) à la performance maximale\n",
    "- Adapter métriques et régularisation au problème spécifique\n",
    "- Automatiser les pipelines tout en conservant la supervision humaine\n",
    "\n",
    "La maîtrise de ces concepts de régularisation constitue un prérequis essentiel pour aborder les techniques de classification avancées et l'optimisation des modèles complexes présentées dans les modules suivants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093986d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
